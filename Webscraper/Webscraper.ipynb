{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf16803",
   "metadata": {},
   "source": [
    "# German news pages Webscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412dde10",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f98f9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import date\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import logging\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import html.parser\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4cb71a",
   "metadata": {},
   "source": [
    "## Get HTML of news pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "82c49e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempts=3\n",
    "loadingWebPage_time_long=10\n",
    "loadingWebPage_time_short=5\n",
    "retry_time=5\n",
    "\n",
    "def create_logfile():\n",
    "    date_time = datetime.datetime.today().strftime('%d-%b-%y_%H:%M:%S')\n",
    "    logfile = f\"/Users/jan/Documents/Python_Projects/Bachelorthesis/log/{date_time}.log\"\n",
    "    logging.basicConfig(filename=logfile, filemode='w', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', force=True)\n",
    "    logging.info(f'Log file {logfile} created')\n",
    "    return logging\n",
    "\n",
    "def create_html_file(html, newsPage_name, logging):\n",
    "    # create file\n",
    "    filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/HTML_Backlog/\"\n",
    "    logging.info(f\"{newsPage_name} Creating html file @ {filepath}\")\n",
    "    dateTime=datetime.datetime.now()\n",
    "    \n",
    "    filename=newsPage_name+\"_\"+dateTime.strftime(\"%d%m%Y_%H_%M_%S\") + \".html\"\n",
    "    logging.info(f\"{newsPage_name}: Creating html file @ {filepath}/{filename}\")\n",
    "    \n",
    "    # delete file if it already exists\n",
    "    logging.info(f\"{newsPage_name}: Check if html already created\")\n",
    "    os.chdir(filepath)\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(file)\n",
    "        logging.info(f\"{newsPage_name}: {filename} deleted\")\n",
    "    else:\n",
    "        logging.info(f\"{newsPage_name}: {filename} doesnt exist yet\")\n",
    "    \n",
    "    with open(filepath+filename,\"w+\") as file:\n",
    "        file.write(str(html))\n",
    "        file.close()\n",
    "    logging.info(f\"{newsPage_name}: Successfully saved file @ {filepath}/{filename}\")\n",
    "    logging.info(\"_______________________________________________________________________\")\n",
    "                \n",
    "        \n",
    "    \n",
    "          \n",
    "def startChromedriver(startUpUrl,logging):\n",
    "\n",
    "    logging.info(f\"Starting Chromedriver with @ {startUpUrl} and loadingTime {loadingWebPage_time_long}s\")\n",
    "    ser = Service(\"/Applications/chromedriver\")\n",
    "\n",
    "    # start chrome driver\n",
    "    driver = webdriver.Chrome(service=ser)\n",
    "    driver.get(startUpUrl)\n",
    "\n",
    "    # wait for page to load\n",
    "    time.sleep(loadingWebPage_time_long)\n",
    "\n",
    "    return driver\n",
    "    \n",
    "    \n",
    "def open_WebPage_AcceptCookies(logging, newsPage_name, url, cookieWindowFrame_XPATH, cookieWindowAcceptButton_XPATH):\n",
    "    loggingInfo=newsPage_name+\": \"\n",
    "    logging.info(f\"{loggingInfo}Start scraping {newsPage_name} news page...\")\n",
    "\n",
    "    logging.info(f\"{loggingInfo}Scraping {newsPage_name} @ {url}\")\n",
    "    \n",
    "    #cookieWindowFrame_XPATH= \"//iframe[contains(@id,'sp_message_iframe_541484')]\"\n",
    "    #cookieWindowAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[1]/button\"\n",
    "    \n",
    "    # starting Chromedriver\n",
    "    driver = startChromedriver(url,logging)\n",
    "    \n",
    "    # switch to CookieWindow (IFrame)\n",
    "    logging.info(f\"{loggingInfo}Switching to {newsPage_name} CookieWindow with XPATH: {cookieWindowFrame_XPATH}\")\n",
    "    \n",
    "    # find iframe\n",
    "    for attempt in range(attempts):        \n",
    "        try:\n",
    "            iframe = driver.find_element(By.XPATH, cookieWindowFrame_XPATH)\n",
    "            logging.info(f\"{loggingInfo}Found cookie window...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{loggingInfo}Can't find cookie window for {newsPage_name} news page\")\n",
    "            logging.error(e)\n",
    "            time.sleep(retry_time)\n",
    "            \n",
    "    for attempt in range(attempts):    \n",
    "        try:\n",
    "            # switch to iframe\n",
    "            driver.switch_to.frame(iframe)\n",
    "    \n",
    "            # accept Cookies\n",
    "            logging.info(f\"{loggingInfo}Accepting Cookies with XPATH: {cookieWindowAcceptButton_XPATH}\")\n",
    "            driver.find_element(By.XPATH, cookieWindowAcceptButton_XPATH).click()\n",
    "            \n",
    "            # switch back to default frame\n",
    "            driver.switch_to.default_content()           \n",
    "            logging.info(f\"{loggingInfo}Accepting cookies successfull\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{loggingInfo} Error while switching to frame and accepting cookies...\")\n",
    "            logging.error(e)\n",
    "            time.sleep(retry_time)\n",
    "            \n",
    "            \n",
    "    return driver\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_Spiegel_HTML(logging, driver):\n",
    "    \n",
    "    # Scroll down to bottom of page\n",
    "    logging.info(f\"Spiegel: Scrolling down to bottom of page\")\n",
    "    driver.find_element(By.CSS_SELECTOR,\"body\").send_keys(Keys.CONTROL, Keys.END);\n",
    "    \n",
    "    # save HTML file\n",
    "    create_html_file(driver.page_source,\"Spiegel\", logging)\n",
    "    \n",
    "    # Close the driver to avoid memory leak errors\n",
    "    driver.quit()\n",
    "\n",
    "    \n",
    "    \n",
    "def get_Sueddeutsche_HTML(logging,driver):\n",
    "    ammountOfPages = 21\n",
    "    # save first page \n",
    "    \n",
    "    for page in range(ammountOfPages):\n",
    "        logging.info(\"Sueddeutsche: Starting to get Sueddeutsche\"+ str(page))\n",
    "        create_html_file(driver.page_source,\"Sueddeutsche\"+ str(page),logging)\n",
    "        \n",
    "        driver.find_element(By.XPATH, \"//*[@id=\\\"paging\\\"]/li[3]/a\").click()\n",
    "        time.sleep(loadingWebPage_time_short)\n",
    "        \n",
    "    # Close the driver to avoid memory leak errors        \n",
    "    driver.quit()\n",
    "        \n",
    "def get_Bild_HTML(logging,driver):\n",
    "    # Scroll down to bottom of page\n",
    "    logging.info(f\"Bild: Scrolling down to bottom of page\")\n",
    "    driver.find_element(By.CSS_SELECTOR,\"body\").send_keys(Keys.CONTROL, Keys.END);\n",
    "    \n",
    "    # save HTML file\n",
    "    create_html_file(driver.page_source,\"Bild\", logging)\n",
    "    \n",
    "    # Close the driver to avoid memory leak errors\n",
    "    driver.quit()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc286fc4",
   "metadata": {},
   "source": [
    "## Scrape news pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "65fc7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvBacklog_filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/CSV_Backlog\"\n",
    "htmlBacklog_filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/HTML_Backlog\"\n",
    "\n",
    "def saveAsCSV(all_news_articles,news_article_labels, filepath):\n",
    "    with open(filepath + \"csv\", \"a\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(news_article_labels)\n",
    "        w.writerows(all_news_articles)\n",
    "        \n",
    "    \n",
    "    #df = pd.DataFrame(all_news_articles)\n",
    "    #df.to_csv(filepath +\"csv\", index=True)\n",
    "\n",
    "def start_Scraping(logging):   \n",
    "    logging.info(\"*** Starting Scraper ***\")\n",
    "    \n",
    "    #get list of csvBacklog and make them compareable to htmlBacklog\n",
    "    logging.info(\"get list of csv Backlog\")\n",
    "    csv_files = os.listdir(csvBacklog_filepath)\n",
    "    logging.info(str(len(csv_files))+ \" files in csv Backlog...\")\n",
    "    csvBacklog_filenames=[]\n",
    "    for file in csv_files:\n",
    "        logging.info(f\"FILENAME: from csv Files: {file}\")\n",
    "        file_name= file.replace(\"csv\",\"html\")\n",
    "        csvBacklog_filenames.append(file_name)\n",
    "    \n",
    "    #get list of htmlBacklog    \n",
    "    logging.info(\"get list of html Backlog\")\n",
    "    html_files = os.listdir(htmlBacklog_filepath)\n",
    "    logging.info(str(len(html_files)) +\" files in html Backlog...\")\n",
    "\n",
    "    # get all html files that are not scraped yet\n",
    "    unscraped_html_files=list(set(html_files) - set(csvBacklog_filenames))\n",
    "    logging.info(str(len(unscraped_html_files)) + \" Html files are not scraped yet\")\n",
    "    \n",
    "    \n",
    "    logging.info(\"Start scraping files...\")\n",
    "    for unscraped_file in unscraped_html_files:\n",
    "        logging.info(f\"Scraping: {unscraped_file}\")\n",
    "        \n",
    "        file_name = str(unscraped_file).replace(htmlBacklog_filepath,\"\")\n",
    "        \n",
    "        if file_name.startswith(\"Spiegel\"):\n",
    "            scrape_Spiegel_NewsPage(logging,unscraped_file)\n",
    "            \n",
    "        elif file_name.startswith(\"Sueddeutsche\"):\n",
    "            scrape_Sueddeutsche_NewsPage(logging,unscraped_file)\n",
    "            \n",
    "        elif file_name.startswith(\"Bild\"):\n",
    "            logging.error(\"BILD Scraper not implemented yet!\")\n",
    "        \n",
    "def scrape_Spiegel_NewsPage(logging, fileName):\n",
    "    logging.info(f\"Starting to Scrape Spiegel file {fileName}\")\n",
    "    # open html file\n",
    "    html_file=open(htmlBacklog_filepath + \"/\" +fileName)\n",
    "    # read html file\n",
    "    htmlSoup=BeautifulSoup(html_file.read(),\"html.parser\")\n",
    "    #f ind all articles\n",
    "    newsElements=htmlSoup.find_all(attrs={\"data-block-el\": \"articleTeaser\"})\n",
    "    all_news_articles=[]\n",
    "    # iterate over all news articles\n",
    "    for newsElement in newsElements:\n",
    "        article= newsElement.find(\"article\")\n",
    "        header=article.find(\"header\").find(\"h2\").find(\"a\")\n",
    "        URL=header.get(\"href\")\n",
    "        Titel=header.get(\"title\")\n",
    "        logging.info(f\"Trying to scrape: {Titel}\")\n",
    "        header=article.find(\"header\")\n",
    "        h2=header.find(\"h2\").find(\"a\")\n",
    "        try: \n",
    "            footer=article.find(\"div\", {\"class\" : \"mt-8 flex items-center justify-between\"})               \n",
    "            footer_Text=footer.text\n",
    "            footer_lst=footer_Text.split(\"•\")\n",
    "            Date_Info=footer_lst[0].replace(\"Uhr\",\"\").replace(\"\\n\",\"\")\n",
    "            Kategorie=footer_lst[1].replace(\"\\n\",\"\")\n",
    "            \n",
    "        except:\n",
    "            footer=article.find(\"footer\", {\"class\" : \"mt-4 inline-block whitespace-nowrap font-sansUI font-normal text-s text-shade-dark dark:text-shade-light\"})               \n",
    "            footer_Text=footer.text\n",
    "            footer_lst=footer_Text.split(\"•\")\n",
    "            Date_Info=footer_lst[0].replace(\"Uhr\",\"\").replace(\"\\n\",\"\")\n",
    "            Kategorie=footer_lst[1].replace(\"\\n\",\"\")\n",
    "            \n",
    "        Zugriff_Datum=str(fileName).replace(htmlBacklog_filepath+ \"/Spiegel_\",\"\").replace(\".html\",\"\")\n",
    "\n",
    "        if \"Dezember\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"Dezember\", \"December\")\n",
    "        if \"Januar\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"Januar\", \"January\")\n",
    "                \n",
    "        file_dateName=fileName.replace(\"Spiegel_\",\"\")        \n",
    "        \n",
    "        if len(Date_Info.strip())==5 and \".\" in Date_Info:\n",
    "            Date_Info=Date_Info.replace(\".\",\":\") \n",
    "            \n",
    "            Date_Info=file_dateName[0:8] + \"_\"+Date_Info\n",
    "\n",
    "        elif \",\" in Date_Info:\n",
    "            Date_Info = Date_Info.replace(\" \",\"\")\n",
    "            Date_Info = Date_Info.replace(\",\",file_dateName[4:8]+\",\",1)\n",
    "            Date_Info = datetime.datetime.strptime(Date_Info, '%d.%B%Y,%H.%M')\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "        \n",
    "        News_page=\"Spiegel\"\n",
    "        news_article = [Titel,URL,Date_Info,News_page]\n",
    "        Date_Info = Date_Info.strip()\n",
    "        news_article_labels=[\"Titel\",\"URL\",\"Date_Info\",\"News_page\"]\n",
    "        all_news_articles.append(news_article)\n",
    "    new_csvFilePath=csvBacklog_filepath+\"/\"+fileName\n",
    "    new_csvFilePath=new_csvFilePath.replace(\"html\",\"\")\n",
    "    saveAsCSV(all_news_articles,news_article_labels,new_csvFilePath)\n",
    "    \n",
    "def scrape_Sueddeutsche_NewsPage(logging,fileName):\n",
    "    logging.info(f\"Starting to Scrape Sueddeutsche file {fileName}\")\n",
    "    all_news_articles=[]\n",
    "    #open html file\n",
    "    html_file=open(htmlBacklog_filepath + \"/\" +fileName)\n",
    "    htmlSoup=BeautifulSoup(html_file.read(),\"html.parser\")\n",
    "    newsElements=htmlSoup.find(\"div\",class_=\"entrylist is-detail\")\n",
    "    newsElements_list=newsElements.find_all(\"div\",class_=\"entrylist__entry\")\n",
    "    \n",
    "    for newsElement in newsElements_list:\n",
    "        timeInfo=newsElement.find(\"time\", class_ =\"entrylist__time\")\n",
    "        logging.info(timeInfo)\n",
    "        Date_Info=timeInfo.text.replace(\"\\n\",\"\").replace(\"vor\",\"\")\n",
    "        newTime= timeInfo.text.replace(\"\\n\",\"\").replace(\"vor\",\"\")\n",
    "\n",
    "        content=newsElement.find(\"div\", class_=\"entrylist__content\")\n",
    "        a=content.find(\"a\",class_=\"entrylist__link\")\n",
    "\n",
    "        URL=a.get(\"href\")\n",
    "        detailedInformations=content.find(\"p\",class_=\"entrylist__detail detailed-information\")\n",
    "        try:\n",
    "            Overline = a.find(\"strong\", class_=\"entrylist__overline\").text\n",
    "        except:\n",
    "            logging.info(\"No Overline found\")\n",
    "            Overline=\"\"\n",
    "        \n",
    "        try:\n",
    "            Titel=a.find(\"em\",class_=\"entrylist__title\").text\n",
    "        except:\n",
    "            Titel=\"\"\n",
    "        try:\n",
    "            singleBreadCrumbItem=content.find(\"span\", class_=\"breadcrumb-list__item\").text\n",
    "            Breadcrumb=singleBreadCrumbItem\n",
    "        except:\n",
    "            logging.info(\"breadcrumb-List__Item not found\")\n",
    "            Breadcrumb=\"\"\n",
    "        try:\n",
    "            BreadCrumb = content.find(\"ul\", class_=\"breadcrumb-list\")\n",
    "            BreadCrumb_list=BreadCrumb.find_all(\"li\",class_=\"breadcrumb-list__item\")\n",
    "            list1=[]\n",
    "            for crumb in BreadCrumb_list:\n",
    "                list1.append(crumb.text.replace(\"\\n\",\"\").strip())\n",
    "            breadcrumbs=\",\".join(list)\n",
    "            Breadcrumb=breadcrumbs\n",
    "        except:\n",
    "            logging.info(\"Breadcrumb-list not found\")\n",
    "\n",
    "        try:\n",
    "            author=detailedInformations.find(\"span\", class_=\"entrylist__author\").text\n",
    "        except:\n",
    "            logging.info(\"No author found\")\n",
    "            author=\"\"\n",
    "\n",
    "        try:\n",
    "            detailed_informations=detailedInformations.text.strip()\n",
    "        except:\n",
    "            logging.info(\"No detailed informations found\")\n",
    "            detailed_informations=\"\"\n",
    "\n",
    "\n",
    "        # create regex to only get the data from filename\n",
    "        pattern = \"(?<=_)(\\d{8})(?=_)\"\n",
    "        regex = re.search(pattern, fileName)\n",
    "        \n",
    "        # create access date\n",
    "        Zugriff_Datum = regex.group().strip()\n",
    "\n",
    "        #Fix Date_Info\n",
    "        Date_Info=Date_Info.strip()\n",
    "        if len(Date_Info) == 5 and \":\" in Date_Info:\n",
    "            Date_Info=Zugriff_Datum + \"_\" + Date_Info\n",
    "            \n",
    "        elif \"|\" in Date_Info:            \n",
    "            Date_Info = datetime.datetime.strptime(Date_Info, \"%d.%m.%Y | %H:%M\")\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "        elif \"Min.\" in Date_Info:\n",
    "            Date_Info = Date_Info.replace(\" Min.\",\"\")\n",
    "            timeDelta = datetime.timedelta(minutes=int(Date_Info))\n",
    "            \n",
    "            Date_Span = regex.span()\n",
    "            \n",
    "            Date_Span = list(Date_Span)      \n",
    "            Date_Span[1]=Date_Span[1]+9\n",
    "            \n",
    "            Date_Info_From_FileName=fileName[int(Date_Span[0]) : int(Date_Span[1])]\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H_%M_%S')\n",
    "            Date_Info_From_FileName=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H:%M')\n",
    "            \n",
    "            Date_Info = Date_Info_From_FileName - timeDelta\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "        elif \"gerade eben\" in Date_Info:\n",
    "            Date_Span = regex.span()\n",
    "            \n",
    "            Date_Span = list(Date_Span)      \n",
    "            Date_Span[1]=Date_Span[1]+9\n",
    "            Date_Info_From_FileName=fileName[int(Date_Span[0]) : int(Date_Span[1])]\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H_%M_%S')\n",
    "            Date_Info_From_FileName=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H:%M')\n",
    "            Date_Info=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "        else:\n",
    "            ErrorCounter.append(Date_Info)\n",
    "        News_page=\"Sueddeutsche\"   \n",
    "        Date_Info = Date_Info.strip()\n",
    "        news_article = [Titel, Date_Info, URL, Overline, Breadcrumb, author, detailed_informations, Zugriff_Datum,News_page]\n",
    "        news_article_labels=[\"Titel\", \"Date_Info\", \"URL\", \"Overline\", \"Breadcrumb\", \"author\", \"detailed_informations\", \"Zugriff_Datum\",\"News_page\"]\n",
    "        all_news_articles.append(news_article)\n",
    "        \n",
    "    new_csvFilePath=csvBacklog_filepath+\"/\"+fileName\n",
    "    new_csvFilePath=new_csvFilePath.replace(\"html\",\"\")\n",
    "    saveAsCSV(all_news_articles, news_article_labels, new_csvFilePath)\n",
    "\n",
    "ErrorCounter=[]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ecaee758",
   "metadata": {},
   "outputs": [],
   "source": [
    "spiegel_url=\"https://www.spiegel.de/schlagzeilen/\"\n",
    "spiegel_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_541484')]\"\n",
    "spiegel_cookieAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[1]/button\"\n",
    "\n",
    "sueddeutsche_url=\"https://www.sueddeutsche.de/news\"\n",
    "sueddeutsche_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_585922')]\"\n",
    "sueddeutsch_cookieAcceptButton_XPATH= \"//*[@id=\\\"notice\\\"]/div[3]/div/div/button[1]\"\n",
    "\n",
    "bild_url = \"https://www.bild.de/home/newsticker/news/alle-news-54190636.bild.html\"\n",
    "bild_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_585666')]\"\n",
    "bild_cookieAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[2]/button\"\n",
    "\n",
    "\n",
    "logging = create_logfile()\n",
    "# Spiegel\n",
    "driver = open_WebPage_AcceptCookies(logging, \"Spiegel\", spiegel_url, spiegel_cookieWindowFrame_XPATH,spiegel_cookieAcceptButton_XPATH)\n",
    "get_Spiegel_HTML(logging,driver)\n",
    "\n",
    "# Sueddeutsche\n",
    "driver = open_WebPage_AcceptCookies(logging,\"Sueddeutsche\", sueddeutsche_url,sueddeutsche_cookieWindowFrame_XPATH, sueddeutsch_cookieAcceptButton_XPATH)\n",
    "get_Sueddeutsche_HTML(logging,driver)\n",
    "\n",
    "# Bild\n",
    "driver =open_WebPage_AcceptCookies(logging,\"Bild\" ,bild_url,bild_cookieWindowFrame_XPATH,bild_cookieAcceptButton_XPATH)\n",
    "get_Bild_HTML(logging,driver)\n",
    "\n",
    "logging.info(\"###################### Downloading news pages HTML finished!... ######################\")\n",
    "start_Scraping(logging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "13b17a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = create_logfile()\n",
    "start_Scraping(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b179247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
