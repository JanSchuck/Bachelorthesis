{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf16803",
   "metadata": {},
   "source": [
    "# German news pages Webscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412dde10",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f98f9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import date\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import logging\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import html.parser\n",
    "\n",
    "import re\n",
    "\n",
    "import schedule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4cb71a",
   "metadata": {},
   "source": [
    "## Get HTML of news pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c49e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempts=3\n",
    "loadingWebPage_time_long=10\n",
    "loadingWebPage_time_short=5\n",
    "retry_time=5\n",
    "\n",
    "def create_logfile():\n",
    "    date_time = datetime.datetime.today().strftime('%d-%b-%y_%H:%M:%S')\n",
    "    logfile = f\"/Users/jan/Documents/Python_Projects/Bachelorthesis/log/{date_time}.log\"\n",
    "    logging.basicConfig(filename=logfile, filemode='w', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', force=True)\n",
    "    logging.info(f'Log file {logfile} created')\n",
    "    return logging\n",
    "\n",
    "def create_html_file(html, newsPage_name, logging):\n",
    "    # create file\n",
    "    filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/HTML_Backlog/\"\n",
    "    logging.info(f\"{newsPage_name} Creating html file @ {filepath}\")\n",
    "    dateTime=datetime.datetime.now()\n",
    "    \n",
    "    filename=newsPage_name+\"_\"+dateTime.strftime(\"%d%m%Y_%H_%M_%S\") + \".html\"\n",
    "    logging.info(f\"{newsPage_name}: Creating html file @ {filepath}/{filename}\")\n",
    "    \n",
    "    # delete file if it already exists\n",
    "    logging.info(f\"{newsPage_name}: Check if html already created\")\n",
    "    os.chdir(filepath)\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(file)\n",
    "        logging.info(f\"{newsPage_name}: {filename} deleted\")\n",
    "    else:\n",
    "        logging.info(f\"{newsPage_name}: {filename} doesnt exist yet\")\n",
    "    \n",
    "    with open(filepath+filename,\"w+\") as file:\n",
    "        file.write(str(html))\n",
    "        file.close()\n",
    "    logging.info(f\"{newsPage_name}: Successfully saved file @ {filepath}/{filename}\")\n",
    "    logging.info(\"_______________________________________________________________________\")\n",
    "                \n",
    "        \n",
    "    \n",
    "          \n",
    "def startChromedriver(startUpUrl,logging):\n",
    "\n",
    "    logging.info(f\"Starting Chromedriver with @ {startUpUrl} and loadingTime {loadingWebPage_time_long}s\")\n",
    "    ser = Service(\"/Applications/chromedriver\")\n",
    "\n",
    "    # start chrome driver\n",
    "    driver = webdriver.Chrome(service=ser)\n",
    "    driver.get(startUpUrl)\n",
    "\n",
    "    # wait for page to load\n",
    "    time.sleep(loadingWebPage_time_long)\n",
    "\n",
    "    return driver\n",
    "    \n",
    "    \n",
    "def open_WebPage_AcceptCookies(logging, newsPage_name, url, cookieWindowFrame_XPATH, cookieWindowAcceptButton_XPATH):\n",
    "    loggingInfo=newsPage_name+\": \"\n",
    "    logging.info(f\"{loggingInfo}Start scraping {newsPage_name} news page...\")\n",
    "\n",
    "    logging.info(f\"{loggingInfo}Scraping {newsPage_name} @ {url}\")\n",
    "    \n",
    "    #cookieWindowFrame_XPATH= \"//iframe[contains(@id,'sp_message_iframe_541484')]\"\n",
    "    #cookieWindowAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[1]/button\"\n",
    "    \n",
    "    # starting Chromedriver\n",
    "    driver = startChromedriver(url,logging)\n",
    "    \n",
    "    # switch to CookieWindow (IFrame)\n",
    "    logging.info(f\"{loggingInfo}Switching to {newsPage_name} CookieWindow with XPATH: {cookieWindowFrame_XPATH}\")\n",
    "    \n",
    "    # find iframe\n",
    "    for attempt in range(attempts):        \n",
    "        try:\n",
    "            iframe = driver.find_element(By.XPATH, cookieWindowFrame_XPATH)\n",
    "            logging.info(f\"{loggingInfo}Found cookie window...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{loggingInfo}Can't find cookie window for {newsPage_name} news page\")\n",
    "            logging.error(e)\n",
    "            time.sleep(retry_time)\n",
    "            \n",
    "    for attempt in range(attempts):    \n",
    "        try:\n",
    "            # switch to iframe\n",
    "            driver.switch_to.frame(iframe)\n",
    "    \n",
    "            # accept Cookies\n",
    "            logging.info(f\"{loggingInfo}Accepting Cookies with XPATH: {cookieWindowAcceptButton_XPATH}\")\n",
    "            driver.find_element(By.XPATH, cookieWindowAcceptButton_XPATH).click()\n",
    "            \n",
    "            # switch back to default frame\n",
    "            driver.switch_to.default_content()           \n",
    "            logging.info(f\"{loggingInfo}Accepting cookies successfull\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{loggingInfo} Error while switching to frame and accepting cookies...\")\n",
    "            logging.error(e)\n",
    "            time.sleep(retry_time)\n",
    "            \n",
    "            \n",
    "    return driver\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_Spiegel_HTML(logging, driver):\n",
    "    \n",
    "    # Scroll down to bottom of page\n",
    "    logging.info(f\"Spiegel: Scrolling down to bottom of page\")\n",
    "    driver.find_element(By.CSS_SELECTOR,\"body\").send_keys(Keys.CONTROL, Keys.END);\n",
    "    \n",
    "    # save HTML file\n",
    "    create_html_file(driver.page_source,\"Spiegel\", logging)\n",
    "    \n",
    "    # Close the driver to avoid memory leak errors\n",
    "    driver.quit()\n",
    "\n",
    "    \n",
    "    \n",
    "def get_Sueddeutsche_HTML(logging,driver):\n",
    "    ammountOfPages = 21\n",
    "    # save first page \n",
    "    \n",
    "    for page in range(ammountOfPages):\n",
    "        logging.info(\"Sueddeutsche: Starting to get Sueddeutsche\"+ str(page))\n",
    "        create_html_file(driver.page_source,\"Sueddeutsche\"+ str(page),logging)\n",
    "        \n",
    "        driver.find_element(By.XPATH, \"//*[@id=\\\"paging\\\"]/li[3]/a\").click()\n",
    "        time.sleep(loadingWebPage_time_short)\n",
    "        \n",
    "    # Close the driver to avoid memory leak errors        \n",
    "    driver.quit()\n",
    "        \n",
    "def get_Bild_HTML(logging,driver):\n",
    "    # Scroll down to bottom of page\n",
    "    logging.info(f\"Bild: Scrolling down to bottom of page\")\n",
    "    driver.find_element(By.CSS_SELECTOR,\"body\").send_keys(Keys.CONTROL, Keys.END);\n",
    "    \n",
    "    # save HTML file\n",
    "    create_html_file(driver.page_source,\"Bild\", logging)\n",
    "    \n",
    "    # Close the driver to avoid memory leak errors\n",
    "    driver.quit()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc286fc4",
   "metadata": {},
   "source": [
    "## Scrape news pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65fc7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvBacklog_filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/CSV_Backlog\"\n",
    "htmlBacklog_filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/HTML_Backlog\"\n",
    "\n",
    "def saveAsCSV(all_news_articles,news_article_labels, filepath):\n",
    "    with open(filepath + \"csv\", \"a\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(news_article_labels)\n",
    "        w.writerows(all_news_articles)\n",
    "        \n",
    "    \n",
    "    #df = pd.DataFrame(all_news_articles)\n",
    "    #df.to_csv(filepath +\"csv\", index=True)\n",
    "\n",
    "def start_Scraping(logging):   \n",
    "    logging.info(\"*** Starting Scraper ***\")\n",
    "    \n",
    "    #get list of csvBacklog and make them compareable to htmlBacklog\n",
    "    logging.info(\"get list of csv Backlog\")\n",
    "    csv_files = os.listdir(csvBacklog_filepath)\n",
    "    logging.info(str(len(csv_files))+ \" files in csv Backlog...\")\n",
    "    csvBacklog_filenames=[]\n",
    "    for file in csv_files:\n",
    "        logging.info(f\"FILENAME: from csv Files: {file}\")\n",
    "        file_name= file.replace(\"csv\",\"html\")\n",
    "        csvBacklog_filenames.append(file_name)\n",
    "    \n",
    "    #get list of htmlBacklog    \n",
    "    logging.info(\"get list of html Backlog\")\n",
    "    html_files = os.listdir(htmlBacklog_filepath)\n",
    "    logging.info(str(len(html_files)) +\" files in html Backlog...\")\n",
    "\n",
    "    # get all html files that are not scraped yet\n",
    "    unscraped_html_files=list(set(html_files) - set(csvBacklog_filenames))\n",
    "    logging.info(str(len(unscraped_html_files)) + \" Html files are not scraped yet\")\n",
    "    \n",
    "    \n",
    "    logging.info(\"Start scraping files...\")\n",
    "    for unscraped_file in unscraped_html_files:\n",
    "        logging.info(f\"Scraping: {unscraped_file}\")\n",
    "        \n",
    "        file_name = str(unscraped_file).replace(htmlBacklog_filepath,\"\")\n",
    "        \n",
    "        if file_name.startswith(\"Spiegel\"):\n",
    "            scrape_Spiegel_NewsPage(logging,unscraped_file)\n",
    "            \n",
    "        elif file_name.startswith(\"Sueddeutsche\"):\n",
    "            scrape_Sueddeutsche_NewsPage(logging,unscraped_file)\n",
    "            \n",
    "        elif file_name.startswith(\"Bild\"):\n",
    "            logging.error(\"BILD Scraper not implemented yet!\")\n",
    "        \n",
    "def scrape_Spiegel_NewsPage(logging, fileName):\n",
    "    logging.info(f\"Starting to Scrape Spiegel file {fileName}\")\n",
    "    # open html file\n",
    "    html_file=open(htmlBacklog_filepath + \"/\" +fileName)\n",
    "    # read html file\n",
    "    htmlSoup=BeautifulSoup(html_file.read(),\"html.parser\")\n",
    "    #f ind all articles\n",
    "    newsElements=htmlSoup.find_all(attrs={\"data-block-el\": \"articleTeaser\"})\n",
    "    all_news_articles=[]\n",
    "    # iterate over all news articles\n",
    "    for newsElement in newsElements:\n",
    "        article= newsElement.find(\"article\")\n",
    "        header=article.find(\"header\").find(\"h2\").find(\"a\")\n",
    "        URL=header.get(\"href\")\n",
    "        Titel=header.get(\"title\")\n",
    "        logging.info(f\"Trying to scrape: {Titel}\")\n",
    "        header=article.find(\"header\")\n",
    "        h2=header.find(\"h2\").find(\"a\")\n",
    "        try: \n",
    "            footer=article.find(\"div\", {\"class\" : \"mt-8 flex items-center justify-between\"})               \n",
    "            footer_Text=footer.text\n",
    "            footer_lst=footer_Text.split(\"•\")\n",
    "            Date_Info=footer_lst[0].replace(\"Uhr\",\"\").replace(\"\\n\",\"\")\n",
    "            Kategorie=footer_lst[1].replace(\"\\n\",\"\")\n",
    "            \n",
    "        except:\n",
    "            footer=article.find(\"footer\", {\"class\" : \"mt-4 inline-block whitespace-nowrap font-sansUI font-normal text-s text-shade-dark dark:text-shade-light\"})               \n",
    "            footer_Text=footer.text\n",
    "            footer_lst=footer_Text.split(\"•\")\n",
    "            Date_Info=footer_lst[0].replace(\"Uhr\",\"\").replace(\"\\n\",\"\")\n",
    "            Kategorie=footer_lst[1].replace(\"\\n\",\"\")\n",
    "            \n",
    "        Zugriff_Datum=str(fileName).replace(htmlBacklog_filepath+ \"/Spiegel_\",\"\").replace(\".html\",\"\")\n",
    "\n",
    "        if \"Dezember\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"Dezember\", \"December\")\n",
    "        if \"Januar\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"Januar\", \"January\")\n",
    "        if \"Februar\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"Februar\", \"February\")   \n",
    "        if \"März\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"März\", \"March\")\n",
    "        \n",
    "        file_dateName=fileName.replace(\"Spiegel_\",\"\")        \n",
    "        \n",
    "        if len(Date_Info.strip())==5 and \".\" in Date_Info:\n",
    "            Date_Info=Date_Info.replace(\".\",\":\") \n",
    "            \n",
    "            Date_Info=file_dateName[0:8] + \"_\"+Date_Info\n",
    "\n",
    "        elif \",\" in Date_Info:\n",
    "            Date_Info = Date_Info.replace(\" \",\"\")\n",
    "            Date_Info = Date_Info.replace(\",\",file_dateName[4:8]+\",\",1)\n",
    "            Date_Info = datetime.datetime.strptime(Date_Info, '%d.%B%Y,%H.%M')\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "            \n",
    "            #Hotfix for wrong dates at year end\n",
    "            if \"31122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"31122022\",\"31122021\")\n",
    "            elif \"30122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"30122022\",\"30122021\")\n",
    "            elif \"29122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"29122022\",\"29122021\")\n",
    "            elif \"28122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"28122022\",\"28122021\")\n",
    "            elif \"27122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"27122022\",\"27122021\")\n",
    "            elif \"26122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"26122022\",\"26122021\")\n",
    "            elif \"25122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"25122022\",\"25122021\")\n",
    "                   \n",
    "        News_page=\"Spiegel\"\n",
    "        news_article = [Titel,URL,Date_Info,News_page]\n",
    "        Date_Info = Date_Info.strip()\n",
    "        news_article_labels=[\"Titel\",\"URL\",\"Date_Info\",\"News_page\"]\n",
    "        all_news_articles.append(news_article)\n",
    "    new_csvFilePath=csvBacklog_filepath+\"/\"+fileName\n",
    "    new_csvFilePath=new_csvFilePath.replace(\"html\",\"\")\n",
    "    saveAsCSV(all_news_articles,news_article_labels,new_csvFilePath)\n",
    "    \n",
    "def scrape_Sueddeutsche_NewsPage(logging,fileName):\n",
    "    logging.info(f\"Starting to Scrape Sueddeutsche file {fileName}\")\n",
    "    all_news_articles=[]\n",
    "    #open html file\n",
    "    html_file=open(htmlBacklog_filepath + \"/\" +fileName)\n",
    "    htmlSoup=BeautifulSoup(html_file.read(),\"html.parser\")\n",
    "    newsElements=htmlSoup.find(\"div\",class_=\"entrylist is-detail\")\n",
    "    newsElements_list=newsElements.find_all(\"div\",class_=\"entrylist__entry\")\n",
    "    \n",
    "    for newsElement in newsElements_list:\n",
    "        timeInfo=newsElement.find(\"time\", class_ =\"entrylist__time\")\n",
    "        logging.info(timeInfo)\n",
    "        Date_Info=timeInfo.text.replace(\"\\n\",\"\").replace(\"vor\",\"\")\n",
    "        newTime= timeInfo.text.replace(\"\\n\",\"\").replace(\"vor\",\"\")\n",
    "\n",
    "        content=newsElement.find(\"div\", class_=\"entrylist__content\")\n",
    "        a=content.find(\"a\",class_=\"entrylist__link\")\n",
    "\n",
    "        URL=a.get(\"href\")\n",
    "        detailedInformations=content.find(\"p\",class_=\"entrylist__detail detailed-information\")\n",
    "        try:\n",
    "            Overline = a.find(\"strong\", class_=\"entrylist__overline\").text\n",
    "        except:\n",
    "            logging.info(\"No Overline found\")\n",
    "            Overline=\"\"\n",
    "        \n",
    "        try:\n",
    "            Titel=a.find(\"em\",class_=\"entrylist__title\").text\n",
    "        except:\n",
    "            Titel=\"\"\n",
    "        try:\n",
    "            singleBreadCrumbItem=content.find(\"span\", class_=\"breadcrumb-list__item\").text\n",
    "            Breadcrumb=singleBreadCrumbItem\n",
    "        except:\n",
    "            logging.info(\"breadcrumb-List__Item not found\")\n",
    "            Breadcrumb=\"\"\n",
    "        try:\n",
    "            BreadCrumb = content.find(\"ul\", class_=\"breadcrumb-list\")\n",
    "            BreadCrumb_list=BreadCrumb.find_all(\"li\",class_=\"breadcrumb-list__item\")\n",
    "            list1=[]\n",
    "            for crumb in BreadCrumb_list:\n",
    "                list1.append(crumb.text.replace(\"\\n\",\"\").strip())\n",
    "            breadcrumbs=\",\".join(list)\n",
    "            Breadcrumb=breadcrumbs\n",
    "        except:\n",
    "            logging.info(\"Breadcrumb-list not found\")\n",
    "\n",
    "        try:\n",
    "            author=detailedInformations.find(\"span\", class_=\"entrylist__author\").text\n",
    "        except:\n",
    "            logging.info(\"No author found\")\n",
    "            author=\"\"\n",
    "\n",
    "        try:\n",
    "            detailed_informations=detailedInformations.text.strip()\n",
    "        except:\n",
    "            logging.info(\"No detailed informations found\")\n",
    "            detailed_informations=\"\"\n",
    "\n",
    "\n",
    "        # create regex to only get the data from filename\n",
    "        pattern = \"(?<=_)(\\d{8})(?=_)\"\n",
    "        regex = re.search(pattern, fileName)\n",
    "        \n",
    "        # create access date\n",
    "        Zugriff_Datum = regex.group().strip()\n",
    "\n",
    "        #Fix Date_Info\n",
    "        Date_Info=Date_Info.strip()\n",
    "        if len(Date_Info) == 5 and \":\" in Date_Info:\n",
    "            Date_Info=Zugriff_Datum + \"_\" + Date_Info\n",
    "            \n",
    "        elif \"|\" in Date_Info:            \n",
    "            Date_Info = datetime.datetime.strptime(Date_Info, \"%d.%m.%Y | %H:%M\")\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "        elif \"Min.\" in Date_Info:\n",
    "            Date_Info = Date_Info.replace(\" Min.\",\"\")\n",
    "            timeDelta = datetime.timedelta(minutes=int(Date_Info))\n",
    "            \n",
    "            Date_Span = regex.span()\n",
    "            \n",
    "            Date_Span = list(Date_Span)      \n",
    "            Date_Span[1]=Date_Span[1]+9\n",
    "            \n",
    "            Date_Info_From_FileName=fileName[int(Date_Span[0]) : int(Date_Span[1])]\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H_%M_%S')\n",
    "            Date_Info_From_FileName=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H:%M')\n",
    "            \n",
    "            Date_Info = Date_Info_From_FileName - timeDelta\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "        elif \"gerade eben\" in Date_Info:\n",
    "            Date_Span = regex.span()\n",
    "            \n",
    "            Date_Span = list(Date_Span)      \n",
    "            Date_Span[1]=Date_Span[1]+9\n",
    "            Date_Info_From_FileName=fileName[int(Date_Span[0]) : int(Date_Span[1])]\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H_%M_%S')\n",
    "            Date_Info_From_FileName=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H:%M')\n",
    "            Date_Info=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "        else:\n",
    "            ErrorCounter.append(Date_Info)\n",
    "        News_page=\"Sueddeutsche\"   \n",
    "        Date_Info = Date_Info.strip()\n",
    "        news_article = [Titel, Date_Info, URL, Overline, Breadcrumb, author, detailed_informations, Zugriff_Datum,News_page]\n",
    "        news_article_labels=[\"Titel\", \"Date_Info\", \"URL\", \"Overline\", \"Breadcrumb\", \"author\", \"detailed_informations\", \"Zugriff_Datum\",\"News_page\"]\n",
    "        all_news_articles.append(news_article)\n",
    "        \n",
    "    new_csvFilePath=csvBacklog_filepath+\"/\"+fileName\n",
    "    new_csvFilePath=new_csvFilePath.replace(\"html\",\"\")\n",
    "    saveAsCSV(all_news_articles, news_article_labels, new_csvFilePath)\n",
    "\n",
    "ErrorCounter=[]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecaee758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAIN():\n",
    "    spiegel_url=\"https://www.spiegel.de/schlagzeilen/\"\n",
    "    spiegel_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_541484')]\"\n",
    "    spiegel_cookieAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[1]/button\"\n",
    "\n",
    "    sueddeutsche_url=\"https://www.sueddeutsche.de/news\"\n",
    "    sueddeutsche_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_596049')]\"\n",
    "    sueddeutsch_cookieAcceptButton_XPATH= \"//*[@id=\\\"notice\\\"]/div[3]/div/div/button[1]\"\n",
    "    bild_url = \"https://www.bild.de/home/newsticker/news/alle-news-54190636.bild.html\"\n",
    "    bild_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_585666')]\"\n",
    "    bild_cookieAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[2]/button\"\n",
    "\n",
    "\n",
    "    logging = create_logfile()\n",
    "    # Spiegel\n",
    "    driver = open_WebPage_AcceptCookies(logging, \"Spiegel\", spiegel_url, spiegel_cookieWindowFrame_XPATH,spiegel_cookieAcceptButton_XPATH)\n",
    "    get_Spiegel_HTML(logging,driver)\n",
    "\n",
    "    # Sueddeutsche\n",
    "    driver = open_WebPage_AcceptCookies(logging,\"Sueddeutsche\", sueddeutsche_url,sueddeutsche_cookieWindowFrame_XPATH, sueddeutsch_cookieAcceptButton_XPATH)\n",
    "    get_Sueddeutsche_HTML(logging,driver)\n",
    "\n",
    "    # Bild\n",
    "    driver =open_WebPage_AcceptCookies(logging,\"Bild\" ,bild_url,bild_cookieWindowFrame_XPATH,bild_cookieAcceptButton_XPATH)\n",
    "    get_Bild_HTML(logging,driver)\n",
    "\n",
    "    logging.info(\"###################### Downloading news pages HTML finished!... ######################\")\n",
    "    start_Scraping(logging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b17a8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-16878f01e646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"08:23\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'schedule' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-16878f01e646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"08:23\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        schedule.every().day.at(\"08:23\").do(MAIN)\n",
    "\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20411a2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '12.März2022,22.26' does not match format '%d.%B%Y,%H.%M'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-99efb065ef3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMAIN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-86f303ac4b65>\u001b[0m in \u001b[0;36mMAIN\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"###################### Downloading news pages HTML finished!... ######################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mstart_Scraping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-0ec3eaf687b9>\u001b[0m in \u001b[0;36mstart_Scraping\u001b[0;34m(logging)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spiegel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mscrape_Spiegel_NewsPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munscraped_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sueddeutsche\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-0ec3eaf687b9>\u001b[0m in \u001b[0;36mscrape_Spiegel_NewsPage\u001b[0;34m(logging, fileName)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mDate_Info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDate_Info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mDate_Info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDate_Info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_dateName\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mDate_Info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDate_Info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%d.%B%Y,%H.%M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mDate_Info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDate_Info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%d%m%Y_%H:%M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    567\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 568\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0m\u001b[1;32m    350\u001b[0m                          (data_string, format))\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data '12.März2022,22.26' does not match format '%d.%B%Y,%H.%M'"
     ]
    }
   ],
   "source": [
    "MAIN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
