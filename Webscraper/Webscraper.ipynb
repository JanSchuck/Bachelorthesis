{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf16803",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# German news pages Webscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412dde10",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f98f9fb5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import date\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import logging\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import html.parser\n",
    "\n",
    "import re\n",
    "\n",
    "import schedule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4cb71a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get HTML of news pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82c49e0d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attempts=3\n",
    "loadingWebPage_time_long=10\n",
    "loadingWebPage_time_short=5\n",
    "retry_time=5\n",
    "\n",
    "def create_logfile():\n",
    "    date_time = datetime.datetime.today().strftime('%d-%b-%y_%H:%M:%S')\n",
    "    logfile = f\"/Users/jan/Documents/Python_Projects/Bachelorthesis/log/{date_time}.log\"\n",
    "    logging.basicConfig(filename=logfile, filemode='w', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', force=True)\n",
    "    logging.info(f'Log file {logfile} created')\n",
    "    return logging\n",
    "\n",
    "def create_html_file(html, newsPage_name, logging):\n",
    "    # create file\n",
    "    filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/HTML_Backlog/\"\n",
    "    logging.info(f\"{newsPage_name} Creating html file @ {filepath}\")\n",
    "    dateTime=datetime.datetime.now()\n",
    "    \n",
    "    filename=newsPage_name+\"_\"+dateTime.strftime(\"%d%m%Y_%H_%M_%S\") + \".html\"\n",
    "    logging.info(f\"{newsPage_name}: Creating html file @ {filepath}/{filename}\")\n",
    "    \n",
    "    # delete file if it already exists\n",
    "    logging.info(f\"{newsPage_name}: Check if html already created\")\n",
    "    os.chdir(filepath)\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(file)\n",
    "        logging.info(f\"{newsPage_name}: {filename} deleted\")\n",
    "    else:\n",
    "        logging.info(f\"{newsPage_name}: {filename} doesnt exist yet\")\n",
    "    \n",
    "    with open(filepath+filename,\"w+\") as file:\n",
    "        file.write(str(html))\n",
    "        file.close()\n",
    "    logging.info(f\"{newsPage_name}: Successfully saved file @ {filepath}/{filename}\")\n",
    "    logging.info(\"_______________________________________________________________________\")\n",
    "                \n",
    "        \n",
    "    \n",
    "          \n",
    "def startChromedriver(startUpUrl,logging):\n",
    "\n",
    "    logging.info(f\"Starting Chromedriver with @ {startUpUrl} and loadingTime {loadingWebPage_time_long}s\")\n",
    "    ser = Service(\"/Applications/chromedriver\")\n",
    "\n",
    "    # start chrome driver\n",
    "    driver = webdriver.Chrome(service=ser)\n",
    "    driver.get(startUpUrl)\n",
    "\n",
    "    # wait for page to load\n",
    "    time.sleep(loadingWebPage_time_long)\n",
    "\n",
    "    return driver\n",
    "    \n",
    "    \n",
    "def open_WebPage_AcceptCookies(logging, newsPage_name, url, cookieWindowFrame_XPATH, cookieWindowAcceptButton_XPATH):\n",
    "    loggingInfo=newsPage_name+\": \"\n",
    "    logging.info(f\"{loggingInfo}Start scraping {newsPage_name} news page...\")\n",
    "\n",
    "    logging.info(f\"{loggingInfo}Scraping {newsPage_name} @ {url}\")\n",
    "    \n",
    "    #cookieWindowFrame_XPATH= \"//iframe[contains(@id,'sp_message_iframe_541484')]\"\n",
    "    #cookieWindowAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[1]/button\"\n",
    "    \n",
    "    # starting Chromedriver\n",
    "    driver = startChromedriver(url,logging)\n",
    "    \n",
    "    # switch to CookieWindow (IFrame)\n",
    "    logging.info(f\"{loggingInfo}Switching to {newsPage_name} CookieWindow with XPATH: {cookieWindowFrame_XPATH}\")\n",
    "    \n",
    "    # find iframe\n",
    "    for attempt in range(attempts):        \n",
    "        try:\n",
    "            iframe = driver.find_element(By.XPATH, cookieWindowFrame_XPATH)\n",
    "            logging.info(f\"{loggingInfo}Found cookie window...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{loggingInfo}Can't find cookie window for {newsPage_name} news page\")\n",
    "            logging.error(e)\n",
    "            time.sleep(retry_time)\n",
    "            \n",
    "    for attempt in range(attempts):    \n",
    "        try:\n",
    "            # switch to iframe\n",
    "            driver.switch_to.frame(iframe)\n",
    "    \n",
    "            # accept Cookies\n",
    "            logging.info(f\"{loggingInfo}Accepting Cookies with XPATH: {cookieWindowAcceptButton_XPATH}\")\n",
    "            driver.find_element(By.XPATH, cookieWindowAcceptButton_XPATH).click()\n",
    "            \n",
    "            # switch back to default frame\n",
    "            driver.switch_to.default_content()           \n",
    "            logging.info(f\"{loggingInfo}Accepting cookies successfull\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{loggingInfo} Error while switching to frame and accepting cookies...\")\n",
    "            logging.error(e)\n",
    "            time.sleep(retry_time)\n",
    "            \n",
    "            \n",
    "    return driver\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_Spiegel_HTML(logging, driver):\n",
    "    \n",
    "    # Scroll down to bottom of page\n",
    "    logging.info(f\"Spiegel: Scrolling down to bottom of page\")\n",
    "    driver.find_element(By.CSS_SELECTOR,\"body\").send_keys(Keys.CONTROL, Keys.END);\n",
    "    \n",
    "    # save HTML file\n",
    "    create_html_file(driver.page_source,\"Spiegel\", logging)\n",
    "    \n",
    "    # Close the driver to avoid memory leak errors\n",
    "    driver.quit()\n",
    "\n",
    "    \n",
    "    \n",
    "def get_Sueddeutsche_HTML(logging,driver):\n",
    "    ammountOfPages = 21\n",
    "    # save first page \n",
    "    \n",
    "    for page in range(ammountOfPages):\n",
    "        logging.info(\"Sueddeutsche: Starting to get Sueddeutsche\"+ str(page))\n",
    "        create_html_file(driver.page_source,\"Sueddeutsche\"+ str(page),logging)\n",
    "        \n",
    "        driver.find_element(By.XPATH, \"//*[@id=\\\"paging\\\"]/li[3]/a\").click()\n",
    "        time.sleep(loadingWebPage_time_short)\n",
    "        \n",
    "    # Close the driver to avoid memory leak errors        \n",
    "    driver.quit()\n",
    "        \n",
    "def get_Bild_HTML(logging,driver):\n",
    "    # Scroll down to bottom of page\n",
    "    logging.info(f\"Bild: Scrolling down to bottom of page\")\n",
    "    driver.find_element(By.CSS_SELECTOR,\"body\").send_keys(Keys.CONTROL, Keys.END);\n",
    "    \n",
    "    # save HTML file\n",
    "    create_html_file(driver.page_source,\"Bild\", logging)\n",
    "    \n",
    "    # Close the driver to avoid memory leak errors\n",
    "    driver.quit()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc286fc4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Scrape news pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65fc7eed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "csvBacklog_filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/CSV_Backlog\"\n",
    "htmlBacklog_filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/HTML_Backlog\"\n",
    "\n",
    "def saveAsCSV(all_news_articles,news_article_labels, filepath):\n",
    "    with open(filepath + \"csv\", \"a\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(news_article_labels)\n",
    "        w.writerows(all_news_articles)\n",
    "        \n",
    "    \n",
    "    #df = pd.DataFrame(all_news_articles)\n",
    "    #df.to_csv(filepath +\"csv\", index=True)\n",
    "\n",
    "def start_Scraping(logging):   \n",
    "    logging.info(\"*** Starting Scraper ***\")\n",
    "    \n",
    "    #get list of csvBacklog and make them compareable to htmlBacklog\n",
    "    logging.info(\"get list of csv Backlog\")\n",
    "    csv_files = os.listdir(csvBacklog_filepath)\n",
    "    logging.info(str(len(csv_files))+ \" files in csv Backlog...\")\n",
    "    csvBacklog_filenames=[]\n",
    "    for file in csv_files:\n",
    "        logging.info(f\"FILENAME: from csv Files: {file}\")\n",
    "        file_name= file.replace(\"csv\",\"html\")\n",
    "        csvBacklog_filenames.append(file_name)\n",
    "    \n",
    "    #get list of htmlBacklog    \n",
    "    logging.info(\"get list of html Backlog\")\n",
    "    html_files = os.listdir(htmlBacklog_filepath)\n",
    "    logging.info(str(len(html_files)) +\" files in html Backlog...\")\n",
    "\n",
    "    # get all html files that are not scraped yet\n",
    "    unscraped_html_files=list(set(html_files) - set(csvBacklog_filenames))\n",
    "    logging.info(str(len(unscraped_html_files)) + \" Html files are not scraped yet\")\n",
    "    \n",
    "    \n",
    "    logging.info(\"Start scraping files...\")\n",
    "    for unscraped_file in unscraped_html_files:\n",
    "        logging.info(f\"Scraping: {unscraped_file}\")\n",
    "        \n",
    "        file_name = str(unscraped_file).replace(htmlBacklog_filepath,\"\")\n",
    "        \n",
    "        if file_name.startswith(\"Spiegel\"):\n",
    "            scrape_Spiegel_NewsPage(logging,unscraped_file)\n",
    "            \n",
    "        elif file_name.startswith(\"Sueddeutsche\"):\n",
    "            scrape_Sueddeutsche_NewsPage(logging,unscraped_file)\n",
    "            \n",
    "        elif file_name.startswith(\"Bild\"):\n",
    "            scrape_Bild_NewsPage(logging,unscraped_file)\n",
    "        \n",
    "def scrape_Spiegel_NewsPage(logging, fileName):\n",
    "    logging.info(f\"Starting to Scrape Spiegel file {fileName}\")\n",
    "    # open html file\n",
    "    html_file=open(htmlBacklog_filepath + \"/\" +fileName)\n",
    "    # read html file\n",
    "    htmlSoup=BeautifulSoup(html_file.read(),\"html.parser\")\n",
    "    #f ind all articles\n",
    "    newsElements=htmlSoup.find_all(attrs={\"data-block-el\": \"articleTeaser\"})\n",
    "    all_news_articles=[]\n",
    "    # iterate over all news articles\n",
    "    for newsElement in newsElements:\n",
    "        article= newsElement.find(\"article\")\n",
    "        header=article.find(\"header\").find(\"h2\").find(\"a\")\n",
    "        URL=header.get(\"href\")\n",
    "        Titel=header.get(\"title\")\n",
    "        logging.info(f\"Trying to scrape: {Titel}\")\n",
    "        header=article.find(\"header\")\n",
    "        h2=header.find(\"h2\").find(\"a\")\n",
    "        try: \n",
    "            footer=article.find(\"div\", {\"class\" : \"mt-8 flex items-center justify-between\"})               \n",
    "            footer_Text=footer.text\n",
    "            footer_lst=footer_Text.split(\"•\")\n",
    "            Date_Info=footer_lst[0].replace(\"Uhr\",\"\").replace(\"\\n\",\"\")\n",
    "            Kategorie=footer_lst[1].replace(\"\\n\",\"\")            \n",
    "        except:\n",
    "            footer=article.find(\"footer\", {\"class\" : \"mt-4 inline-block whitespace-nowrap font-sansUI font-normal text-s text-shade-dark dark:text-shade-light\"})               \n",
    "            footer_Text=footer.text\n",
    "            footer_lst=footer_Text.split(\"•\")\n",
    "            Date_Info=footer_lst[0].replace(\"Uhr\",\"\").replace(\"\\n\",\"\")\n",
    "            Kategorie=footer_lst[1].replace(\"\\n\",\"\")\n",
    "            \n",
    "            \n",
    "        Zugriff_Datum=str(fileName).replace(htmlBacklog_filepath+ \"/Spiegel_\",\"\").replace(\".html\",\"\")\n",
    "\n",
    "        if \"Dezember\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"Dezember\", \"December\")\n",
    "        if \"Januar\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"Januar\", \"January\")\n",
    "        if \"Februar\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"Februar\", \"February\")   \n",
    "        if \"März\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"März\", \"March\")\n",
    "        \n",
    "        file_dateName=fileName.replace(\"Spiegel_\",\"\")        \n",
    "        \n",
    "        if len(Date_Info.strip())==5 and \".\" in Date_Info:\n",
    "            Date_Info=Date_Info.replace(\".\",\":\") \n",
    "            \n",
    "            Date_Info=file_dateName[0:8] + \"_\"+Date_Info\n",
    "\n",
    "        elif \",\" in Date_Info:\n",
    "            Date_Info = Date_Info.replace(\" \",\"\")\n",
    "            Date_Info = Date_Info.replace(\",\",file_dateName[4:8]+\",\",1)\n",
    "            Date_Info = datetime.datetime.strptime(Date_Info, '%d.%B%Y,%H.%M')\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "            \n",
    "            #Hotfix for wrong dates at year end\n",
    "            if \"31122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"31122022\",\"31122021\")\n",
    "            elif \"30122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"30122022\",\"30122021\")\n",
    "            elif \"29122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"29122022\",\"29122021\")\n",
    "            elif \"28122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"28122022\",\"28122021\")\n",
    "            elif \"27122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"27122022\",\"27122021\")\n",
    "            elif \"26122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"26122022\",\"26122021\")\n",
    "            elif \"25122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"25122022\",\"25122021\")\n",
    "                   \n",
    "        News_page=\"Spiegel\"\n",
    "        news_article = [Titel,URL,Date_Info,News_page,Kategorie]\n",
    "        Date_Info = Date_Info.strip()\n",
    "        news_article_labels=[\"Titel\",\"URL\",\"Date_Info\",\"News_page\",\"Kategorie\"]\n",
    "        all_news_articles.append(news_article)\n",
    "    new_csvFilePath=csvBacklog_filepath+\"/\"+fileName\n",
    "    new_csvFilePath=new_csvFilePath.replace(\"html\",\"\")\n",
    "    saveAsCSV(all_news_articles,news_article_labels,new_csvFilePath)\n",
    "\n",
    "def scrape_Sueddeutsche_NewsPage(logging,fileName):\n",
    "    logging.info(f\"Starting to Scrape Sueddeutsche file {fileName}\")\n",
    "    all_news_articles=[]\n",
    "    #open html file\n",
    "    html_file=open(htmlBacklog_filepath + \"/\" +fileName)\n",
    "    htmlSoup=BeautifulSoup(html_file.read(),\"html.parser\")\n",
    "    newsElements=htmlSoup.find(\"div\",class_=\"entrylist is-detail\")\n",
    "    newsElements_list=newsElements.find_all(\"div\",class_=\"entrylist__entry\")\n",
    "\n",
    "    for newsElement in newsElements_list:\n",
    "        timeInfo=newsElement.find(\"time\", class_ =\"entrylist__time\")\n",
    "        logging.info(timeInfo)\n",
    "        Date_Info=timeInfo.text.replace(\"\\n\",\"\").replace(\"vor\",\"\")\n",
    "        newTime= timeInfo.text.replace(\"\\n\",\"\").replace(\"vor\",\"\")\n",
    "\n",
    "        content=newsElement.find(\"div\", class_=\"entrylist__content\")\n",
    "        a=content.find(\"a\",class_=\"entrylist__link\")\n",
    "\n",
    "        URL=a.get(\"href\")\n",
    "        detailedInformations=content.find(\"p\",class_=\"entrylist__detail detailed-information\")\n",
    "        try:\n",
    "            Overline = a.find(\"strong\", class_=\"entrylist__overline\").text\n",
    "        except:\n",
    "            logging.info(\"No Overline found\")\n",
    "            Overline=\"\"\n",
    "\n",
    "        try:\n",
    "            Titel=a.find(\"em\",class_=\"entrylist__title\").text\n",
    "        except:\n",
    "            Titel=\"\"\n",
    "        try:\n",
    "            singleBreadCrumbItem=content.find(\"span\", class_=\"breadcrumb-list__item\").text\n",
    "            Breadcrumb=singleBreadCrumbItem\n",
    "        except:\n",
    "            logging.info(\"breadcrumb-List__Item not found\")\n",
    "            Breadcrumb=\"\"\n",
    "        try:\n",
    "            BreadCrumb = content.find(\"ul\", class_=\"breadcrumb-list\")\n",
    "            BreadCrumb_list=BreadCrumb.find_all(\"li\",class_=\"breadcrumb-list__item\")\n",
    "            list1=[]\n",
    "            for crumb in BreadCrumb_list:\n",
    "                list1.append(crumb.text.replace(\"\\n\",\"\").strip())\n",
    "            breadcrumbs=\",\".join(list)\n",
    "            Breadcrumb=breadcrumbs\n",
    "        except:\n",
    "            logging.info(\"Breadcrumb-list not found\")\n",
    "\n",
    "        try:\n",
    "            author=detailedInformations.find(\"span\", class_=\"entrylist__author\").text\n",
    "        except:\n",
    "            logging.info(\"No author found\")\n",
    "            author=\"\"\n",
    "\n",
    "        try:\n",
    "            detailed_informations=detailedInformations.text.strip()\n",
    "        except:\n",
    "            logging.info(\"No detailed informations found\")\n",
    "            detailed_informations=\"\"\n",
    "\n",
    "\n",
    "        # create regex to only get the data from filename\n",
    "        pattern = \"(?<=_)(\\d{8})(?=_)\"\n",
    "        regex = re.search(pattern, fileName)\n",
    "\n",
    "        # create access date\n",
    "        Zugriff_Datum = regex.group().strip()\n",
    "\n",
    "        #Fix Date_Info\n",
    "        Date_Info=Date_Info.strip()\n",
    "        if len(Date_Info) == 5 and \":\" in Date_Info:\n",
    "            Date_Info=Zugriff_Datum + \"_\" + Date_Info\n",
    "\n",
    "        elif \"|\" in Date_Info:\n",
    "            Date_Info = datetime.datetime.strptime(Date_Info, \"%d.%m.%Y | %H:%M\")\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "        elif \"Min.\" in Date_Info:\n",
    "            Date_Info = Date_Info.replace(\" Min.\",\"\")\n",
    "            timeDelta = datetime.timedelta(minutes=int(Date_Info))\n",
    "\n",
    "            Date_Span = regex.span()\n",
    "\n",
    "            Date_Span = list(Date_Span)\n",
    "            Date_Span[1]=Date_Span[1]+9\n",
    "\n",
    "            Date_Info_From_FileName=fileName[int(Date_Span[0]) : int(Date_Span[1])]\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H_%M_%S')\n",
    "            Date_Info_From_FileName=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H:%M')\n",
    "\n",
    "            Date_Info = Date_Info_From_FileName - timeDelta\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "        elif \"gerade eben\" in Date_Info:\n",
    "            Date_Span = regex.span()\n",
    "\n",
    "            Date_Span = list(Date_Span)\n",
    "            Date_Span[1]=Date_Span[1]+9\n",
    "            Date_Info_From_FileName=fileName[int(Date_Span[0]) : int(Date_Span[1])]\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H_%M_%S')\n",
    "            Date_Info_From_FileName=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H:%M')\n",
    "            Date_Info=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "        else:\n",
    "            ErrorCounter.append(Date_Info)\n",
    "        News_page=\"Sueddeutsche\"\n",
    "        Date_Info = Date_Info.strip()\n",
    "        news_article = [Titel, Date_Info, URL, Overline, Breadcrumb, author, detailed_informations, Zugriff_Datum,News_page]\n",
    "        news_article_labels=[\"Titel\", \"Date_Info\", \"URL\", \"Overline\", \"Breadcrumb\", \"author\", \"detailed_informations\", \"Zugriff_Datum\",\"News_page\"]\n",
    "        all_news_articles.append(news_article)\n",
    "    new_csvFilePath=csvBacklog_filepath+\"/\"+fileName\n",
    "    new_csvFilePath=new_csvFilePath.replace(\"html\",\"\")\n",
    "    saveAsCSV(all_news_articles, news_article_labels, new_csvFilePath)\n",
    "\n",
    "def scrape_Bild_NewsPage(logging, fileName):\n",
    "    htmlBacklog_filepath = \"C:/Users/Jan/Documents/Python_Projects/Bachelorthesis/Bachelorthesis/HTML_Backlog\"\n",
    "    logging.info(f\"Starting to Scrape Bild file {fileName}\")\n",
    "    all_news_article = []\n",
    "\n",
    "    # create regex to only get the data from filename\n",
    "    pattern = \"(?<=_)(\\d{8})(?=_)\"\n",
    "    regex = re.search(pattern, fileName)\n",
    "\n",
    "    # create access date\n",
    "    Zugriff_Datum = regex.group().strip()\n",
    "\n",
    "    # open html htmlBacklog_filepath\n",
    "\n",
    "    with open(htmlBacklog_filepath + \"/\" +fileName, encoding='utf8') as infile:\n",
    "        htmlSoup = BeautifulSoup(infile, \"html.parser\")\n",
    "\n",
    "    # get all news Elements\n",
    "    newsElements_list = []\n",
    "    for i in range(40):\n",
    "        if i == 0:\n",
    "            news_item_list = htmlSoup.find_all(\"div\", class_=\"hentry overlay t10ont posLast\")\n",
    "        else:\n",
    "            news_item_list.extend(htmlSoup.find_all(\"div\", class_=f\"hentry overlay t10ont pos{i}\"))\n",
    "\n",
    "    for newsElement in news_item_list:\n",
    "        a = newsElement.find(\"a\")\n",
    "        URL=a.get(\"href\")\n",
    "\n",
    "        Overline = a.get(\"data-tb-title\")\n",
    "        kicker = a.get(\"data-tb-kicker\")\n",
    "        time = newsElement.find(\"time\")[\"datetime\"]\n",
    "        categorie = a.find(\"li\", class_=\"channel\").text\n",
    "        entry_content = newsElement.find(\"p\", class_=\"entry-content\").text\n",
    "        try:\n",
    "            bild_plus = newsElement.find(\"li\", class_= \"premium bildplus hide-text\").text\n",
    "            if not bild_plus:\n",
    "                bild_plus = False\n",
    "            else:\n",
    "                bild_plus = True\n",
    "        except:\n",
    "            bild_plus = False\n",
    "\n",
    "        # clean time\n",
    "        time = time.replace(\"+02:00\",\"\").replace(\"T\",\" \")\n",
    "        News_page = \"Bild\"\n",
    "        news_article = [Overline, kicker, time, categorie, entry_content, bild_plus, Zugriff_Datum, News_page]\n",
    "        news_article_labels=[\"Overline\", \"kicker\", \"time\", \"categorie\", \"entry_content\", \"bild_plus\", \"Zugriff_Datum\", \"News_page\"]\n",
    "    all_news_article.append(news_article)\n",
    "    new_csvFilePath=csvBacklog_filepath+\"/\"+fileName\n",
    "    new_csvFilePath=new_csvFilePath.replace(\"html\",\"\")\n",
    "    saveAsCSV(all_news_article, news_article_labels, new_csvFilePath)\n",
    "\n",
    "\n",
    "ErrorCounter=[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19d4e337",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [45]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m         news_item_list\u001B[38;5;241m.\u001B[39mextend(htmlSoup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m\"\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhentry overlay t10ont pos\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m newsElement \u001B[38;5;129;01min\u001B[39;00m news_item_list:\n\u001B[0;32m     20\u001B[0m     a \u001B[38;5;241m=\u001B[39m newsElement\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     21\u001B[0m     URL\u001B[38;5;241m=\u001B[39ma\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhref\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Input \u001B[1;32mIn [45]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m         news_item_list\u001B[38;5;241m.\u001B[39mextend(htmlSoup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m\"\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhentry overlay t10ont pos\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m newsElement \u001B[38;5;129;01min\u001B[39;00m news_item_list:\n\u001B[0;32m     20\u001B[0m     a \u001B[38;5;241m=\u001B[39m newsElement\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     21\u001B[0m     URL\u001B[38;5;241m=\u001B[39ma\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhref\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1095\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1053\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2022.1.1\\plugins\\python-ce\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[0;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[1;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2022.1.1\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1155\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1152\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1155\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2022.1.1\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1170\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1167\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1169\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1170\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1172\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1174\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecaee758",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def MAIN():\n",
    "    spiegel_url=\"https://www.spiegel.de/schlagzeilen/\"\n",
    "    spiegel_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_541484')]\"\n",
    "    spiegel_cookieAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[1]/button\"\n",
    "\n",
    "    sueddeutsche_url=\"https://www.sueddeutsche.de/news\"\n",
    "    sueddeutsche_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_596049')]\"\n",
    "    sueddeutsch_cookieAcceptButton_XPATH= \"//*[@id=\\\"notice\\\"]/div[3]/div/div/button[1]\"\n",
    "    bild_url = \"https://www.bild.de/home/newsticker/news/alle-news-54190636.bild.html\"\n",
    "    bild_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_585666')]\"\n",
    "    bild_cookieAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[2]/button\"\n",
    "\n",
    "\n",
    "    logging = create_logfile()\n",
    "    # Spiegel\n",
    "    driver = open_WebPage_AcceptCookies(logging, \"Spiegel\", spiegel_url, spiegel_cookieWindowFrame_XPATH,spiegel_cookieAcceptButton_XPATH)\n",
    "    get_Spiegel_HTML(logging,driver)\n",
    "\n",
    "    # Sueddeutsche\n",
    "    driver = open_WebPage_AcceptCookies(logging,\"Sueddeutsche\", sueddeutsche_url,sueddeutsche_cookieWindowFrame_XPATH, sueddeutsch_cookieAcceptButton_XPATH)\n",
    "    get_Sueddeutsche_HTML(logging,driver)\n",
    "\n",
    "    # Bild\n",
    "    driver =open_WebPage_AcceptCookies(logging,\"Bild\" ,bild_url,bild_cookieWindowFrame_XPATH,bild_cookieAcceptButton_XPATH)\n",
    "    get_Bild_HTML(logging,driver)\n",
    "\n",
    "    logging.info(\"###################### Downloading news pages HTML finished!... ######################\")\n",
    "    start_Scraping(logging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d46e1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Scrape only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25a64478",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'C:\\\\Users\\\\jan\\\\Documents\\\\Python_Projects\\\\Bachelorthesis\\\\log\\\\29-Jun-22_09:44:02.log'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Input \u001B[1;32mIn [51]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m logging \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_logfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m start_Scraping(logging)\n",
      "Input \u001B[1;32mIn [49]\u001B[0m, in \u001B[0;36mcreate_logfile\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m date_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mtoday()\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mb-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124my_\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mS\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      8\u001B[0m logfile \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/jan/Documents/Python_Projects/Bachelorthesis/log/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdate_time\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.log\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 9\u001B[0m \u001B[43mlogging\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbasicConfig\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilemode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mw\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogging\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mINFO\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m%(asctime)s\u001B[39;49;00m\u001B[38;5;124;43m - \u001B[39;49m\u001B[38;5;132;43;01m%(levelname)s\u001B[39;49;00m\u001B[38;5;124;43m - \u001B[39;49m\u001B[38;5;132;43;01m%(message)s\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatefmt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m%d\u001B[39;49;00m\u001B[38;5;124;43m-\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mb-\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43my \u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mH:\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mM:\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mS\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLog file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlogfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m created\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m logging\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\logging\\__init__.py:2003\u001B[0m, in \u001B[0;36mbasicConfig\u001B[1;34m(**kwargs)\u001B[0m\n\u001B[0;32m   2001\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   2002\u001B[0m         errors \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 2003\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[43mFileHandler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2004\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2006\u001B[0m     stream \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\logging\\__init__.py:1146\u001B[0m, in \u001B[0;36mFileHandler.__init__\u001B[1;34m(self, filename, mode, encoding, delay, errors)\u001B[0m\n\u001B[0;32m   1144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1145\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1146\u001B[0m     StreamHandler\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\logging\\__init__.py:1175\u001B[0m, in \u001B[0;36mFileHandler._open\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1171\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1172\u001B[0m \u001B[38;5;124;03m    Open the current base file with the (original) mode and encoding.\u001B[39;00m\n\u001B[0;32m   1173\u001B[0m \u001B[38;5;124;03m    Return the resulting stream.\u001B[39;00m\n\u001B[0;32m   1174\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1175\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbaseFilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1176\u001B[0m \u001B[43m                \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOSError\u001B[0m: [Errno 22] Invalid argument: 'C:\\\\Users\\\\jan\\\\Documents\\\\Python_Projects\\\\Bachelorthesis\\\\log\\\\29-Jun-22_09:44:02.log'"
     ]
    }
   ],
   "source": [
    "logging = create_logfile()\n",
    "start_Scraping(logging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57857988",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-8ade28791a03>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mlogging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_logfile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mscrape_Sueddeutsche_NewsPage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"Sueddeutsche19_22032022_13_03_06.html\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-12-64737b619ec3>\u001B[0m in \u001B[0;36mscrape_Sueddeutsche_NewsPage\u001B[0;34m(logging, fileName)\u001B[0m\n\u001B[1;32m    138\u001B[0m     \u001B[0mhtmlSoup\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mBeautifulSoup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhtml_file\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"html.parser\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0mnewsElements\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhtmlSoup\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"div\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mclass_\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"entrylist is-detail\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 140\u001B[0;31m     \u001B[0mnewsElements_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnewsElements\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_all\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"div\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mclass_\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"entrylist__entry\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    141\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    142\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mnewsElement\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mnewsElements_list\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "logging = create_logfile()\n",
    "scrape_Sueddeutsche_NewsPage(logging,\"Sueddeutsche19_22032022_13_03_06.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b17a8d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        schedule.every().day.at(\"08:23\").do(MAIN)\n",
    "\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20411a2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489589c-0079-47af-891b-758ad64a233a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}