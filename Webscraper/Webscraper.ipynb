{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf16803",
   "metadata": {},
   "source": [
    "# German news pages Webscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412dde10",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98f9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import date\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import logging\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import html.parser\n",
    "\n",
    "import re\n",
    "\n",
    "import schedule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4cb71a",
   "metadata": {},
   "source": [
    "## Get HTML of news pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c49e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempts=3\n",
    "loadingWebPage_time_long=10\n",
    "loadingWebPage_time_short=5\n",
    "retry_time=5\n",
    "\n",
    "def create_logfile():\n",
    "    date_time = datetime.datetime.today().strftime('%d-%b-%y_%H:%M:%S')\n",
    "    logfile = f\"/Users/jan/Documents/Python_Projects/Bachelorthesis/log/{date_time}.log\"\n",
    "    logging.basicConfig(filename=logfile, filemode='w', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', force=True)\n",
    "    logging.info(f'Log file {logfile} created')\n",
    "    return logging\n",
    "\n",
    "def create_html_file(html, newsPage_name, logging):\n",
    "    # create file\n",
    "    filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/HTML_Backlog/\"\n",
    "    logging.info(f\"{newsPage_name} Creating html file @ {filepath}\")\n",
    "    dateTime=datetime.datetime.now()\n",
    "    \n",
    "    filename=newsPage_name+\"_\"+dateTime.strftime(\"%d%m%Y_%H_%M_%S\") + \".html\"\n",
    "    logging.info(f\"{newsPage_name}: Creating html file @ {filepath}/{filename}\")\n",
    "    \n",
    "    # delete file if it already exists\n",
    "    logging.info(f\"{newsPage_name}: Check if html already created\")\n",
    "    os.chdir(filepath)\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(file)\n",
    "        logging.info(f\"{newsPage_name}: {filename} deleted\")\n",
    "    else:\n",
    "        logging.info(f\"{newsPage_name}: {filename} doesnt exist yet\")\n",
    "    \n",
    "    with open(filepath+filename,\"w+\") as file:\n",
    "        file.write(str(html))\n",
    "        file.close()\n",
    "    logging.info(f\"{newsPage_name}: Successfully saved file @ {filepath}/{filename}\")\n",
    "    logging.info(\"_______________________________________________________________________\")\n",
    "                \n",
    "        \n",
    "    \n",
    "          \n",
    "def startChromedriver(startUpUrl,logging):\n",
    "\n",
    "    logging.info(f\"Starting Chromedriver with @ {startUpUrl} and loadingTime {loadingWebPage_time_long}s\")\n",
    "    ser = Service(\"/Applications/chromedriver\")\n",
    "\n",
    "    # start chrome driver\n",
    "    driver = webdriver.Chrome(service=ser)\n",
    "    driver.get(startUpUrl)\n",
    "\n",
    "    # wait for page to load\n",
    "    time.sleep(loadingWebPage_time_long)\n",
    "\n",
    "    return driver\n",
    "    \n",
    "    \n",
    "def open_WebPage_AcceptCookies(logging, newsPage_name, url, cookieWindowFrame_XPATH, cookieWindowAcceptButton_XPATH):\n",
    "    loggingInfo=newsPage_name+\": \"\n",
    "    logging.info(f\"{loggingInfo}Start scraping {newsPage_name} news page...\")\n",
    "\n",
    "    logging.info(f\"{loggingInfo}Scraping {newsPage_name} @ {url}\")\n",
    "    \n",
    "    #cookieWindowFrame_XPATH= \"//iframe[contains(@id,'sp_message_iframe_541484')]\"\n",
    "    #cookieWindowAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[1]/button\"\n",
    "    \n",
    "    # starting Chromedriver\n",
    "    driver = startChromedriver(url,logging)\n",
    "    \n",
    "    # switch to CookieWindow (IFrame)\n",
    "    logging.info(f\"{loggingInfo}Switching to {newsPage_name} CookieWindow with XPATH: {cookieWindowFrame_XPATH}\")\n",
    "    \n",
    "    # find iframe\n",
    "    for attempt in range(attempts):        \n",
    "        try:\n",
    "            iframe = driver.find_element(By.XPATH, cookieWindowFrame_XPATH)\n",
    "            logging.info(f\"{loggingInfo}Found cookie window...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{loggingInfo}Can't find cookie window for {newsPage_name} news page\")\n",
    "            logging.error(e)\n",
    "            time.sleep(retry_time)\n",
    "            \n",
    "    for attempt in range(attempts):    \n",
    "        try:\n",
    "            # switch to iframe\n",
    "            driver.switch_to.frame(iframe)\n",
    "    \n",
    "            # accept Cookies\n",
    "            logging.info(f\"{loggingInfo}Accepting Cookies with XPATH: {cookieWindowAcceptButton_XPATH}\")\n",
    "            driver.find_element(By.XPATH, cookieWindowAcceptButton_XPATH).click()\n",
    "            \n",
    "            # switch back to default frame\n",
    "            driver.switch_to.default_content()           \n",
    "            logging.info(f\"{loggingInfo}Accepting cookies successfull\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{loggingInfo} Error while switching to frame and accepting cookies...\")\n",
    "            logging.error(e)\n",
    "            time.sleep(retry_time)\n",
    "            \n",
    "            \n",
    "    return driver\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_Spiegel_HTML(logging, driver):\n",
    "    \n",
    "    # Scroll down to bottom of page\n",
    "    logging.info(f\"Spiegel: Scrolling down to bottom of page\")\n",
    "    driver.find_element(By.CSS_SELECTOR,\"body\").send_keys(Keys.CONTROL, Keys.END);\n",
    "    \n",
    "    # save HTML file\n",
    "    create_html_file(driver.page_source,\"Spiegel\", logging)\n",
    "    \n",
    "    # Close the driver to avoid memory leak errors\n",
    "    driver.quit()\n",
    "\n",
    "    \n",
    "    \n",
    "def get_Sueddeutsche_HTML(logging,driver):\n",
    "    ammountOfPages = 21\n",
    "    # save first page \n",
    "    \n",
    "    for page in range(ammountOfPages):\n",
    "        logging.info(\"Sueddeutsche: Starting to get Sueddeutsche\"+ str(page))\n",
    "        create_html_file(driver.page_source,\"Sueddeutsche\"+ str(page),logging)\n",
    "        \n",
    "        driver.find_element(By.XPATH, \"//*[@id=\\\"paging\\\"]/li[3]/a\").click()\n",
    "        time.sleep(loadingWebPage_time_short)\n",
    "        \n",
    "    # Close the driver to avoid memory leak errors        \n",
    "    driver.quit()\n",
    "        \n",
    "def get_Bild_HTML(logging,driver):\n",
    "    # Scroll down to bottom of page\n",
    "    logging.info(f\"Bild: Scrolling down to bottom of page\")\n",
    "    driver.find_element(By.CSS_SELECTOR,\"body\").send_keys(Keys.CONTROL, Keys.END);\n",
    "    \n",
    "    # save HTML file\n",
    "    create_html_file(driver.page_source,\"Bild\", logging)\n",
    "    \n",
    "    # Close the driver to avoid memory leak errors\n",
    "    driver.quit()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc286fc4",
   "metadata": {},
   "source": [
    "## Scrape news pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65fc7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvBacklog_filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/CSV_Backlog\"\n",
    "htmlBacklog_filepath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/HTML_Backlog\"\n",
    "\n",
    "def saveAsCSV(all_news_articles,news_article_labels, filepath):\n",
    "    with open(filepath + \"csv\", \"a\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(news_article_labels)\n",
    "        w.writerows(all_news_articles)\n",
    "        \n",
    "    \n",
    "    #df = pd.DataFrame(all_news_articles)\n",
    "    #df.to_csv(filepath +\"csv\", index=True)\n",
    "\n",
    "def start_Scraping(logging):   \n",
    "    logging.info(\"*** Starting Scraper ***\")\n",
    "    \n",
    "    #get list of csvBacklog and make them compareable to htmlBacklog\n",
    "    logging.info(\"get list of csv Backlog\")\n",
    "    csv_files = os.listdir(csvBacklog_filepath)\n",
    "    logging.info(str(len(csv_files))+ \" files in csv Backlog...\")\n",
    "    csvBacklog_filenames=[]\n",
    "    for file in csv_files:\n",
    "        logging.info(f\"FILENAME: from csv Files: {file}\")\n",
    "        file_name= file.replace(\"csv\",\"html\")\n",
    "        csvBacklog_filenames.append(file_name)\n",
    "    \n",
    "    #get list of htmlBacklog    \n",
    "    logging.info(\"get list of html Backlog\")\n",
    "    html_files = os.listdir(htmlBacklog_filepath)\n",
    "    logging.info(str(len(html_files)) +\" files in html Backlog...\")\n",
    "\n",
    "    # get all html files that are not scraped yet\n",
    "    unscraped_html_files=list(set(html_files) - set(csvBacklog_filenames))\n",
    "    logging.info(str(len(unscraped_html_files)) + \" Html files are not scraped yet\")\n",
    "    \n",
    "    \n",
    "    logging.info(\"Start scraping files...\")\n",
    "    for unscraped_file in unscraped_html_files:\n",
    "        logging.info(f\"Scraping: {unscraped_file}\")\n",
    "        \n",
    "        file_name = str(unscraped_file).replace(htmlBacklog_filepath,\"\")\n",
    "        \n",
    "        if file_name.startswith(\"Spiegel\"):\n",
    "            scrape_Spiegel_NewsPage(logging,unscraped_file)\n",
    "            \n",
    "        elif file_name.startswith(\"Sueddeutsche\"):\n",
    "            scrape_Sueddeutsche_NewsPage(logging,unscraped_file)\n",
    "            \n",
    "        elif file_name.startswith(\"Bild\"):\n",
    "            logging.error(\"BILD Scraper not implemented yet!\")\n",
    "        \n",
    "def scrape_Spiegel_NewsPage(logging, fileName):\n",
    "    logging.info(f\"Starting to Scrape Spiegel file {fileName}\")\n",
    "    # open html file\n",
    "    html_file=open(htmlBacklog_filepath + \"/\" +fileName)\n",
    "    # read html file\n",
    "    htmlSoup=BeautifulSoup(html_file.read(),\"html.parser\")\n",
    "    #f ind all articles\n",
    "    newsElements=htmlSoup.find_all(attrs={\"data-block-el\": \"articleTeaser\"})\n",
    "    all_news_articles=[]\n",
    "    # iterate over all news articles\n",
    "    for newsElement in newsElements:\n",
    "        article= newsElement.find(\"article\")\n",
    "        header=article.find(\"header\").find(\"h2\").find(\"a\")\n",
    "        URL=header.get(\"href\")\n",
    "        Titel=header.get(\"title\")\n",
    "        logging.info(f\"Trying to scrape: {Titel}\")\n",
    "        header=article.find(\"header\")\n",
    "        h2=header.find(\"h2\").find(\"a\")\n",
    "        try: \n",
    "            footer=article.find(\"div\", {\"class\" : \"mt-8 flex items-center justify-between\"})               \n",
    "            footer_Text=footer.text\n",
    "            footer_lst=footer_Text.split(\"•\")\n",
    "            Date_Info=footer_lst[0].replace(\"Uhr\",\"\").replace(\"\\n\",\"\")\n",
    "            Kategorie=footer_lst[1].replace(\"\\n\",\"\")\n",
    "            \n",
    "        except:\n",
    "            footer=article.find(\"footer\", {\"class\" : \"mt-4 inline-block whitespace-nowrap font-sansUI font-normal text-s text-shade-dark dark:text-shade-light\"})               \n",
    "            footer_Text=footer.text\n",
    "            footer_lst=footer_Text.split(\"•\")\n",
    "            Date_Info=footer_lst[0].replace(\"Uhr\",\"\").replace(\"\\n\",\"\")\n",
    "            Kategorie=footer_lst[1].replace(\"\\n\",\"\")\n",
    "            \n",
    "        Zugriff_Datum=str(fileName).replace(htmlBacklog_filepath+ \"/Spiegel_\",\"\").replace(\".html\",\"\")\n",
    "\n",
    "        if \"Dezember\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"Dezember\", \"December\")\n",
    "        if \"Januar\" in Date_Info:\n",
    "                Date_Info=Date_Info.replace(\"Januar\", \"January\")\n",
    "                \n",
    "        file_dateName=fileName.replace(\"Spiegel_\",\"\")        \n",
    "        \n",
    "        if len(Date_Info.strip())==5 and \".\" in Date_Info:\n",
    "            Date_Info=Date_Info.replace(\".\",\":\") \n",
    "            \n",
    "            Date_Info=file_dateName[0:8] + \"_\"+Date_Info\n",
    "\n",
    "        elif \",\" in Date_Info:\n",
    "            Date_Info = Date_Info.replace(\" \",\"\")\n",
    "            Date_Info = Date_Info.replace(\",\",file_dateName[4:8]+\",\",1)\n",
    "            Date_Info = datetime.datetime.strptime(Date_Info, '%d.%B%Y,%H.%M')\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "            \n",
    "            #Hotfix for wrong dates at year end\n",
    "            if \"31122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"31122022\",\"31122021\")\n",
    "            elif \"30122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"30122022\",\"30122021\")\n",
    "            elif \"29122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"29122022\",\"29122021\")\n",
    "            elif \"28122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"28122022\",\"28122021\")\n",
    "            elif \"27122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"27122022\",\"27122021\")\n",
    "            elif \"26122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"26122022\",\"26122021\")\n",
    "            elif \"25122022\"  in Date_Info:\n",
    "                Date_Info = Date_Info.replace(\"25122022\",\"25122021\")\n",
    "                   \n",
    "        News_page=\"Spiegel\"\n",
    "        news_article = [Titel,URL,Date_Info,News_page]\n",
    "        Date_Info = Date_Info.strip()\n",
    "        news_article_labels=[\"Titel\",\"URL\",\"Date_Info\",\"News_page\"]\n",
    "        all_news_articles.append(news_article)\n",
    "    new_csvFilePath=csvBacklog_filepath+\"/\"+fileName\n",
    "    new_csvFilePath=new_csvFilePath.replace(\"html\",\"\")\n",
    "    saveAsCSV(all_news_articles,news_article_labels,new_csvFilePath)\n",
    "    \n",
    "def scrape_Sueddeutsche_NewsPage(logging,fileName):\n",
    "    logging.info(f\"Starting to Scrape Sueddeutsche file {fileName}\")\n",
    "    all_news_articles=[]\n",
    "    #open html file\n",
    "    html_file=open(htmlBacklog_filepath + \"/\" +fileName)\n",
    "    htmlSoup=BeautifulSoup(html_file.read(),\"html.parser\")\n",
    "    newsElements=htmlSoup.find(\"div\",class_=\"entrylist is-detail\")\n",
    "    newsElements_list=newsElements.find_all(\"div\",class_=\"entrylist__entry\")\n",
    "    \n",
    "    for newsElement in newsElements_list:\n",
    "        timeInfo=newsElement.find(\"time\", class_ =\"entrylist__time\")\n",
    "        logging.info(timeInfo)\n",
    "        Date_Info=timeInfo.text.replace(\"\\n\",\"\").replace(\"vor\",\"\")\n",
    "        newTime= timeInfo.text.replace(\"\\n\",\"\").replace(\"vor\",\"\")\n",
    "\n",
    "        content=newsElement.find(\"div\", class_=\"entrylist__content\")\n",
    "        a=content.find(\"a\",class_=\"entrylist__link\")\n",
    "\n",
    "        URL=a.get(\"href\")\n",
    "        detailedInformations=content.find(\"p\",class_=\"entrylist__detail detailed-information\")\n",
    "        try:\n",
    "            Overline = a.find(\"strong\", class_=\"entrylist__overline\").text\n",
    "        except:\n",
    "            logging.info(\"No Overline found\")\n",
    "            Overline=\"\"\n",
    "        \n",
    "        try:\n",
    "            Titel=a.find(\"em\",class_=\"entrylist__title\").text\n",
    "        except:\n",
    "            Titel=\"\"\n",
    "        try:\n",
    "            singleBreadCrumbItem=content.find(\"span\", class_=\"breadcrumb-list__item\").text\n",
    "            Breadcrumb=singleBreadCrumbItem\n",
    "        except:\n",
    "            logging.info(\"breadcrumb-List__Item not found\")\n",
    "            Breadcrumb=\"\"\n",
    "        try:\n",
    "            BreadCrumb = content.find(\"ul\", class_=\"breadcrumb-list\")\n",
    "            BreadCrumb_list=BreadCrumb.find_all(\"li\",class_=\"breadcrumb-list__item\")\n",
    "            list1=[]\n",
    "            for crumb in BreadCrumb_list:\n",
    "                list1.append(crumb.text.replace(\"\\n\",\"\").strip())\n",
    "            breadcrumbs=\",\".join(list)\n",
    "            Breadcrumb=breadcrumbs\n",
    "        except:\n",
    "            logging.info(\"Breadcrumb-list not found\")\n",
    "\n",
    "        try:\n",
    "            author=detailedInformations.find(\"span\", class_=\"entrylist__author\").text\n",
    "        except:\n",
    "            logging.info(\"No author found\")\n",
    "            author=\"\"\n",
    "\n",
    "        try:\n",
    "            detailed_informations=detailedInformations.text.strip()\n",
    "        except:\n",
    "            logging.info(\"No detailed informations found\")\n",
    "            detailed_informations=\"\"\n",
    "\n",
    "\n",
    "        # create regex to only get the data from filename\n",
    "        pattern = \"(?<=_)(\\d{8})(?=_)\"\n",
    "        regex = re.search(pattern, fileName)\n",
    "        \n",
    "        # create access date\n",
    "        Zugriff_Datum = regex.group().strip()\n",
    "\n",
    "        #Fix Date_Info\n",
    "        Date_Info=Date_Info.strip()\n",
    "        if len(Date_Info) == 5 and \":\" in Date_Info:\n",
    "            Date_Info=Zugriff_Datum + \"_\" + Date_Info\n",
    "            \n",
    "        elif \"|\" in Date_Info:            \n",
    "            Date_Info = datetime.datetime.strptime(Date_Info, \"%d.%m.%Y | %H:%M\")\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "        elif \"Min.\" in Date_Info:\n",
    "            Date_Info = Date_Info.replace(\" Min.\",\"\")\n",
    "            timeDelta = datetime.timedelta(minutes=int(Date_Info))\n",
    "            \n",
    "            Date_Span = regex.span()\n",
    "            \n",
    "            Date_Span = list(Date_Span)      \n",
    "            Date_Span[1]=Date_Span[1]+9\n",
    "            \n",
    "            Date_Info_From_FileName=fileName[int(Date_Span[0]) : int(Date_Span[1])]\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H_%M_%S')\n",
    "            Date_Info_From_FileName=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H:%M')\n",
    "            \n",
    "            Date_Info = Date_Info_From_FileName - timeDelta\n",
    "            Date_Info=Date_Info.strftime('%d%m%Y_%H:%M')\n",
    "        elif \"gerade eben\" in Date_Info:\n",
    "            Date_Span = regex.span()\n",
    "            \n",
    "            Date_Span = list(Date_Span)      \n",
    "            Date_Span[1]=Date_Span[1]+9\n",
    "            Date_Info_From_FileName=fileName[int(Date_Span[0]) : int(Date_Span[1])]\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H_%M_%S')\n",
    "            Date_Info_From_FileName=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "            Date_Info_From_FileName=datetime.datetime.strptime(Date_Info_From_FileName,'%d%m%Y_%H:%M')\n",
    "            Date_Info=Date_Info_From_FileName.strftime('%d%m%Y_%H:%M')\n",
    "        else:\n",
    "            ErrorCounter.append(Date_Info)\n",
    "        News_page=\"Sueddeutsche\"   \n",
    "        Date_Info = Date_Info.strip()\n",
    "        news_article = [Titel, Date_Info, URL, Overline, Breadcrumb, author, detailed_informations, Zugriff_Datum,News_page]\n",
    "        news_article_labels=[\"Titel\", \"Date_Info\", \"URL\", \"Overline\", \"Breadcrumb\", \"author\", \"detailed_informations\", \"Zugriff_Datum\",\"News_page\"]\n",
    "        all_news_articles.append(news_article)\n",
    "        \n",
    "    new_csvFilePath=csvBacklog_filepath+\"/\"+fileName\n",
    "    new_csvFilePath=new_csvFilePath.replace(\"html\",\"\")\n",
    "    saveAsCSV(all_news_articles, news_article_labels, new_csvFilePath)\n",
    "\n",
    "ErrorCounter=[]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecaee758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAIN():\n",
    "    spiegel_url=\"https://www.spiegel.de/schlagzeilen/\"\n",
    "    spiegel_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_541484')]\"\n",
    "    spiegel_cookieAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[1]/button\"\n",
    "\n",
    "    sueddeutsche_url=\"https://www.sueddeutsche.de/news\"\n",
    "    sueddeutsche_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_596049')]\"\n",
    "    sueddeutsch_cookieAcceptButton_XPATH= \"//*[@id=\\\"notice\\\"]/div[3]/div/div/button[1]\"\n",
    "    bild_url = \"https://www.bild.de/home/newsticker/news/alle-news-54190636.bild.html\"\n",
    "    bild_cookieWindowFrame_XPATH = \"//iframe[contains(@id,'sp_message_iframe_585666')]\"\n",
    "    bild_cookieAcceptButton_XPATH = \"//*[@id=\\\"notice\\\"]/div[3]/div[2]/button\"\n",
    "\n",
    "\n",
    "    logging = create_logfile()\n",
    "    # Spiegel\n",
    "    driver = open_WebPage_AcceptCookies(logging, \"Spiegel\", spiegel_url, spiegel_cookieWindowFrame_XPATH,spiegel_cookieAcceptButton_XPATH)\n",
    "    get_Spiegel_HTML(logging,driver)\n",
    "\n",
    "    # Sueddeutsche\n",
    "    driver = open_WebPage_AcceptCookies(logging,\"Sueddeutsche\", sueddeutsche_url,sueddeutsche_cookieWindowFrame_XPATH, sueddeutsch_cookieAcceptButton_XPATH)\n",
    "    get_Sueddeutsche_HTML(logging,driver)\n",
    "\n",
    "    # Bild\n",
    "    driver =open_WebPage_AcceptCookies(logging,\"Bild\" ,bild_url,bild_cookieWindowFrame_XPATH,bild_cookieAcceptButton_XPATH)\n",
    "    get_Bild_HTML(logging,driver)\n",
    "\n",
    "    logging.info(\"###################### Downloading news pages HTML finished!... ######################\")\n",
    "    start_Scraping(logging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13b17a8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: window was already closed\n  (Session info: chrome=98.0.4758.80)\nStacktrace:\n0   chromedriver                        0x000000010505c280 chromedriver + 4833920\n1   chromedriver                        0x0000000104ff0bf8 chromedriver + 4393976\n2   chromedriver                        0x0000000104be6c84 chromedriver + 158852\n3   chromedriver                        0x0000000104bd7b28 chromedriver + 97064\n4   chromedriver                        0x0000000104bd24fc chromedriver + 75004\n5   chromedriver                        0x0000000104c3f918 chromedriver + 522520\n6   chromedriver                        0x0000000104c0b7e0 chromedriver + 309216\n7   chromedriver                        0x000000010501e828 chromedriver + 4581416\n8   chromedriver                        0x0000000105033450 chromedriver + 4666448\n9   chromedriver                        0x0000000105037d1c chromedriver + 4685084\n10  chromedriver                        0x0000000105033c28 chromedriver + 4668456\n11  chromedriver                        0x0000000105014610 chromedriver + 4539920\n12  chromedriver                        0x000000010504d82c chromedriver + 4773932\n13  chromedriver                        0x000000010504d9a0 chromedriver + 4774304\n14  chromedriver                        0x0000000105062e44 chromedriver + 4861508\n15  libsystem_pthread.dylib             0x00000001b474d240 _pthread_start + 148\n16  libsystem_pthread.dylib             0x00000001b4748024 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b09c09cd8660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[0;34m()\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mdefault_scheduler\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     \"\"\"\n\u001b[0;32m--> 780\u001b[0;31m     \u001b[0mdefault_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mrunnable_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunnable_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay_seconds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36m_run_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Job\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelJob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mCancelJob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running job %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schedule_next_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-86f303ac4b65>\u001b[0m in \u001b[0;36mMAIN\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Spiegel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_WebPage_AcceptCookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Spiegel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspiegel_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspiegel_cookieWindowFrame_XPATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspiegel_cookieAcceptButton_XPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mget_Spiegel_HTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Sueddeutsche\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a4a7fb23e0bd>\u001b[0m in \u001b[0;36mget_Spiegel_HTML\u001b[0;34m(logging, driver)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# Scroll down to bottom of page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Spiegel: Scrolling down to bottom of page\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTROL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# save HTML file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[name=\"%s\"]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0m\u001b[1;32m   1245\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             'value': value})['value']\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    426\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_KT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_VT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_KT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_VT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_VT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: window was already closed\n  (Session info: chrome=98.0.4758.80)\nStacktrace:\n0   chromedriver                        0x000000010505c280 chromedriver + 4833920\n1   chromedriver                        0x0000000104ff0bf8 chromedriver + 4393976\n2   chromedriver                        0x0000000104be6c84 chromedriver + 158852\n3   chromedriver                        0x0000000104bd7b28 chromedriver + 97064\n4   chromedriver                        0x0000000104bd24fc chromedriver + 75004\n5   chromedriver                        0x0000000104c3f918 chromedriver + 522520\n6   chromedriver                        0x0000000104c0b7e0 chromedriver + 309216\n7   chromedriver                        0x000000010501e828 chromedriver + 4581416\n8   chromedriver                        0x0000000105033450 chromedriver + 4666448\n9   chromedriver                        0x0000000105037d1c chromedriver + 4685084\n10  chromedriver                        0x0000000105033c28 chromedriver + 4668456\n11  chromedriver                        0x0000000105014610 chromedriver + 4539920\n12  chromedriver                        0x000000010504d82c chromedriver + 4773932\n13  chromedriver                        0x000000010504d9a0 chromedriver + 4774304\n14  chromedriver                        0x0000000105062e44 chromedriver + 4861508\n15  libsystem_pthread.dylib             0x00000001b474d240 _pthread_start + 148\n16  libsystem_pthread.dylib             0x00000001b4748024 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "schedule.every().day.at(\"12:00\").do(MAIN)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20411a2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '15.Februar2022,23.09' does not match format '%d.%B%Y,%H.%M'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-99efb065ef3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMAIN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-86f303ac4b65>\u001b[0m in \u001b[0;36mMAIN\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"###################### Downloading news pages HTML finished!... ######################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mstart_Scraping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-758d5b4a69d6>\u001b[0m in \u001b[0;36mstart_Scraping\u001b[0;34m(logging)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spiegel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mscrape_Spiegel_NewsPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munscraped_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sueddeutsche\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-758d5b4a69d6>\u001b[0m in \u001b[0;36mscrape_Spiegel_NewsPage\u001b[0;34m(logging, fileName)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mDate_Info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDate_Info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mDate_Info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDate_Info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_dateName\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mDate_Info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDate_Info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%d.%B%Y,%H.%M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mDate_Info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDate_Info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%d%m%Y_%H:%M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    567\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 568\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0m\u001b[1;32m    350\u001b[0m                          (data_string, format))\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data '15.Februar2022,23.09' does not match format '%d.%B%Y,%H.%M'"
     ]
    }
   ],
   "source": [
    "MAIN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
