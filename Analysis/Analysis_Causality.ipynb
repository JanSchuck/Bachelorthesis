{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#adf test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "#granger causality test\\n\",\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load DataFrames"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "path = \"C:/Users/Jan/Documents/Python_Projects/Bachelorthesis/Bachelorthesis/Analysis/DataFrames/\"\n",
    "Occurence_DataFrame = pd.read_csv(path+\"Occurence_DataFrame.csv\", index_col=None,header=0)\n",
    "\n",
    "Occurence_DataFrame_Spiegel = pd.read_csv(path+\"Occurence_DataFrame_Spiegel.csv\", index_col=None,header=0)\n",
    "Occurence_DataFrame_Bild = pd.read_csv(path+\"Occurence_DataFrame_Bild.csv\", index_col=None,header=0)\n",
    "Occurence_DataFrame_Sueddeutsche = pd.read_csv(path+\"Occurence_DataFrame_Sueddeutsche.csv\", index_col=None,header=0)\n",
    "Occurence_DataFrame_Spring = pd.read_csv(path+\"Occurence_DataFrame_spring.csv\", index_col=None,header=0)\n",
    "Occurence_DataFrame_Summer = pd.read_csv(path+\"Occurence_DataFrame_summer.csv\", index_col=None,header=0)\n",
    "Occurence_DataFrame_Winter = pd.read_csv(path+\"Occurence_DataFrame_winter.csv\", index_col=None,header=0)\n",
    "\n",
    "\n",
    "Granger_DataFrame = pd.read_csv(path+\"Granger_DataFrame.csv\", index_col=None,header=0)\n",
    "Granger_DataFrame_Spiegel = pd.read_csv(path+\"Granger_DataFrame_Spiegel.csv\", index_col=None,header=0)\n",
    "Granger_DataFrame_Bild = pd.read_csv(path+\"Granger_DataFrame_Bild.csv\", index_col=None,header=0)\n",
    "Granger_DataFrame_Sueddeutsche = pd.read_csv(path+\"Granger_DataFrame_Sueddeutsche.csv\", index_col=None,header=0)\n",
    "\n",
    "#Granger_DataFrame_Winter = pd.read_csv(path+\"Granger_DataFrame_winter.csv\", index_col=None,header=0)\n",
    "#Granger_DataFrame_Spring = pd.read_csv(path+\"Granger_DataFrame_spring.csv\", index_col=None,header=0)\n",
    "#Granger_DataFrame_Summer = pd.read_csv(path+\"Granger_DataFrame_summer.csv\", index_col=None,header=0)\n",
    "\n",
    "\n",
    "Granger_Distribution = pd.read_csv(path + \"Granger_Distribution.csv\",index_col=None,header=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Keywords with static time series"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Granger_DataFrame_Spring' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m static_keyWords_Bild \u001B[38;5;241m=\u001B[39m Granger_DataFrame_Bild\u001B[38;5;241m.\u001B[39mloc[Granger_DataFrame_Bild[ADF_KPSS_test_result_columns]\u001B[38;5;241m.\u001B[39mall(\u001B[38;5;241m1\u001B[39m)]\n\u001B[0;32m      6\u001B[0m static_keyWords_Sueddeutsche\u001B[38;5;241m=\u001B[39m Granger_DataFrame_Sueddeutsche\u001B[38;5;241m.\u001B[39mloc[Granger_DataFrame_Sueddeutsche[ADF_KPSS_test_result_columns]\u001B[38;5;241m.\u001B[39mall(\u001B[38;5;241m1\u001B[39m)]\n\u001B[1;32m----> 8\u001B[0m static_keyWords_Spring\u001B[38;5;241m=\u001B[39m \u001B[43mGranger_DataFrame_Spring\u001B[49m\u001B[38;5;241m.\u001B[39mloc[Granger_DataFrame_Spring[ADF_KPSS_test_result_columns]\u001B[38;5;241m.\u001B[39mall(\u001B[38;5;241m1\u001B[39m)]\n\u001B[0;32m      9\u001B[0m static_keyWords_Summer\u001B[38;5;241m=\u001B[39m Granger_DataFrame_Summer\u001B[38;5;241m.\u001B[39mloc[Granger_DataFrame_Summer[ADF_KPSS_test_result_columns]\u001B[38;5;241m.\u001B[39mall(\u001B[38;5;241m1\u001B[39m)]\n\u001B[0;32m     10\u001B[0m static_keyWords_Winter\u001B[38;5;241m=\u001B[39m Granger_DataFrame_Winter\u001B[38;5;241m.\u001B[39mloc[Granger_DataFrame_Winter[ADF_KPSS_test_result_columns]\u001B[38;5;241m.\u001B[39mall(\u001B[38;5;241m1\u001B[39m)]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Granger_DataFrame_Spring' is not defined"
     ]
    }
   ],
   "source": [
    "ADF_KPSS_test_result_columns = [\"ADF_stationary_wiki_after_diff\",\"ADF_stationary_news_after_diff\",\"ADF_stationary_google_after_diff\",\"KPSS_stationary_wiki_after_diff\",\"KPSS_stationary_news_after_diff\",\"KPSS_stationary_google_after_diff\"]\n",
    "static_keyWords_all = Granger_DataFrame.loc[Granger_DataFrame[ADF_KPSS_test_result_columns].all(1)]\n",
    "\n",
    "static_keyWords_Spiegel = Granger_DataFrame_Spiegel.loc[Granger_DataFrame_Spiegel[ADF_KPSS_test_result_columns].all(1)]\n",
    "static_keyWords_Bild = Granger_DataFrame_Bild.loc[Granger_DataFrame_Bild[ADF_KPSS_test_result_columns].all(1)]\n",
    "static_keyWords_Sueddeutsche= Granger_DataFrame_Sueddeutsche.loc[Granger_DataFrame_Sueddeutsche[ADF_KPSS_test_result_columns].all(1)]\n",
    "\n",
    "static_keyWords_Spring= Granger_DataFrame_Spring.loc[Granger_DataFrame_Spring[ADF_KPSS_test_result_columns].all(1)]\n",
    "static_keyWords_Summer= Granger_DataFrame_Summer.loc[Granger_DataFrame_Summer[ADF_KPSS_test_result_columns].all(1)]\n",
    "static_keyWords_Winter= Granger_DataFrame_Winter.loc[Granger_DataFrame_Winter[ADF_KPSS_test_result_columns].all(1)]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Keywords with static time series and granger cause"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lowest_lag_columns = [\"lowest_lag_after_diff_News_grangerCauses_Wikipedia\",\"lowest_lag_after_diff_Wikipedia_grangerCauses_News\",\"lowest_lag_after_diff_News_grangerCauses_Google\",\"lowest_lag_after_diff_Google_grangerCauses_News\",\"lowest_lag_after_diff_Google_grangerCauses_Wikipedia\",\"lowest_lag_after_diff_Wikipedia_grangerCauses_Google\"]\n",
    "\n",
    "real_granger_DataFrame = static_keyWords_all.dropna(subset =lowest_lag_columns,thresh=1 )\n",
    "real_granger_DataFrame_Spiegel = static_keyWords_Spiegel.dropna(subset =lowest_lag_columns,thresh=1 )\n",
    "real_granger_DataFrame_Bild = static_keyWords_Bild.dropna(subset =lowest_lag_columns,thresh=1 )\n",
    "real_granger_DataFrame_Sueddeutsche = static_keyWords_Sueddeutsche.dropna(subset =lowest_lag_columns,thresh=1 )\n",
    "\n",
    "real_granger_DataFrame_Spring = static_keyWords_Spring.dropna(subset =lowest_lag_columns,thresh=1 )\n",
    "real_granger_DataFrame_Summer = static_keyWords_Summer.dropna(subset =lowest_lag_columns,thresh=1 )\n",
    "real_granger_DataFrame_Winter = static_keyWords_Winter.dropna(subset =lowest_lag_columns,thresh=1 )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Keywords where news granger cause google"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "news_granger_causes_google = real_granger_DataFrame.dropna(subset=\"lowest_lag_after_diff_News_grangerCauses_Google\")[[\"keyword_name\",\"lowest_lag_after_diff_News_grangerCauses_Google\"]]\n",
    "\n",
    "print(news_granger_causes_google[\"lowest_lag_after_diff_News_grangerCauses_Google\"].describe())\n",
    "print(news_granger_causes_google.info())\n",
    "\n",
    "saveDF_as_CSV(news_granger_causes_google,\"News_Granger_causes_Google\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Keywords where news granger cause wikipedia"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "news_granger_causes_Wiki = real_granger_DataFrame.dropna(subset=\"lowest_lag_after_diff_News_grangerCauses_Wikipedia\")[[\"keyword_name\",\"lowest_lag_after_diff_News_grangerCauses_Wikipedia\"]]\n",
    "\n",
    "print(news_granger_causes_Wiki[\"lowest_lag_after_diff_News_grangerCauses_Wikipedia\"].describe())\n",
    "print(news_granger_causes_Wiki.info())\n",
    "\n",
    "saveDF_as_CSV(news_granger_causes_Wiki,\"News_Granger_causes_Wiki\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Which KeyWords are/not mutual granger causing themselfs\n",
    " News -> Google         / Google -> News\n",
    "    News -> Wikipedia      / Wikipedia -> News,\n",
    "    Google -> Wikipedia    / Wikipedia -> Googel\n",
    "\n",
    "Erkenntnis: Häufig wechselseitige Granger Kausalität -> Die beiden Zeitreihen beeinflussen sich gegenseitig\n",
    "-> jetzt interessant -> Welche Zeitreihen sind nicht wechselseitig"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'real_granger_DataFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 90>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     69\u001B[0m     data \u001B[38;5;241m=\u001B[39m [[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnews_granger_causes_google\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mlen\u001B[39m(news_granger_causes_google)],\n\u001B[0;32m     70\u001B[0m             [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgoogle_granger_causes_news\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mlen\u001B[39m(google_granger_causes_news)],\n\u001B[0;32m     71\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     84\u001B[0m             [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot_mutual_google_wikipedia\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mlen\u001B[39m(not_mutual_google_wikipedia)],\n\u001B[0;32m     85\u001B[0m             [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot_mutual_wikipedia_google\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mlen\u001B[39m(not_mutual_wikipedia_google)]]\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mDataFrame(data, columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mConnection_Granger\u001B[39m\u001B[38;5;124m'\u001B[39m, media_house])\n\u001B[1;32m---> 90\u001B[0m granger_distribution_all \u001B[38;5;241m=\u001B[39m calculate_granger_distribution(\u001B[43mreal_granger_DataFrame\u001B[49m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     92\u001B[0m \u001B[38;5;66;03m# By media house\u001B[39;00m\n\u001B[0;32m     93\u001B[0m granger_distribution_Spiegel \u001B[38;5;241m=\u001B[39m calculate_granger_distribution(real_granger_DataFrame_Spiegel,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpiegel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'real_granger_DataFrame' is not defined"
     ]
    }
   ],
   "source": [
    "# Erstellen einer Liste von granger keywords\n",
    "# aussortieren der wechelseitigen Beziehungen -> kommen in news granger causes google und umgedreht vor\n",
    "\n",
    "# Anzahl der übrig gebliebenen zeigt in welche Richtung es häufiger grangert\n",
    "\n",
    "# ERKENNTNISS: Einfluss von Google und Wikipedia auf News ist größer!\n",
    "\n",
    "def Diff_list(li1, li2):\n",
    "    return list(set(li1) - set(li2))\n",
    "\n",
    "# Erstellen liste von allen keywords bei denen der granger test die nullhypothse ablehnt (es grangert)\n",
    "def calculate_granger_distribution(granger_dataFrame,media_house):\n",
    "    # News causes Google\n",
    "    news_granger_causes_google = granger_dataFrame.dropna(subset=\"lowest_lag_after_diff_News_grangerCauses_Google\")[[\"keyword_name\",\"lowest_lag_after_diff_News_grangerCauses_Google\"]]\n",
    "    google_granger_causes_news = granger_dataFrame.dropna(subset=\"lowest_lag_after_diff_Google_grangerCauses_News\")[[\"keyword_name\",\"lowest_lag_after_diff_Google_grangerCauses_News\"]]\n",
    "\n",
    "    # News causes Wikipedia\n",
    "    news_granger_causes_Wiki = granger_dataFrame.dropna(subset=\"lowest_lag_after_diff_News_grangerCauses_Wikipedia\")[[\"keyword_name\",\"lowest_lag_after_diff_News_grangerCauses_Wikipedia\"]]\n",
    "    Wiki_granger_causes_News = granger_dataFrame.dropna(subset=\"lowest_lag_after_diff_Wikipedia_grangerCauses_News\")[[\"keyword_name\",\"lowest_lag_after_diff_Wikipedia_grangerCauses_News\"]]\n",
    "\n",
    "    # Google causes Wikipedia\n",
    "    Google_granger_causes_Wikipedia = granger_dataFrame.dropna(subset=\"lowest_lag_after_diff_Google_grangerCauses_Wikipedia\")[[\"keyword_name\",\"lowest_lag_after_diff_Google_grangerCauses_Wikipedia\"]]\n",
    "    Wikipedia_granger_causes_Google = granger_dataFrame.dropna(subset=\"lowest_lag_after_diff_Wikipedia_grangerCauses_Google\")[[\"keyword_name\",\"lowest_lag_after_diff_Wikipedia_grangerCauses_Google\"]]\n",
    "\n",
    "    # News - Google\n",
    "    news_google_list = news_granger_causes_google.keyword_name.tolist()\n",
    "    google_news_list = google_granger_causes_news.keyword_name.to_list()\n",
    "    # keywords that are not mutual from news causes google\n",
    "    not_mutual_news_google = Diff_list(news_google_list,google_news_list)\n",
    "    # keywords that are not mutual from google causes news\n",
    "    not_mutual_google_news = Diff_list(google_news_list,news_google_list)\n",
    "    mutual_news_google = list([i for i in news_google_list if i in google_news_list])\n",
    "\n",
    "    # News - Wikipedia\n",
    "    news_wikipedia_list = news_granger_causes_Wiki.keyword_name.to_list()\n",
    "    wikipedia_news_list = Wiki_granger_causes_News.keyword_name.to_list()\n",
    "    # keywords that are not mutual from news causes wikipedia\n",
    "    not_mutual_news_wikipedia = Diff_list(news_wikipedia_list,wikipedia_news_list)\n",
    "    # keywords that are not mutual from google causes news\n",
    "    not_mutual_wikipedia_news = Diff_list(wikipedia_news_list,news_wikipedia_list)\n",
    "    mutual_news_wikipedia = list([i for i in news_wikipedia_list if i in wikipedia_news_list])\n",
    "\n",
    "    # Google - Wikipedia\n",
    "    google_wikipedia_list = Google_granger_causes_Wikipedia.keyword_name.tolist()\n",
    "    wikipedia_google_list = Wikipedia_granger_causes_Google.keyword_name.tolist()\n",
    "    # keywords that are not mutual from google causes wikipedia\n",
    "    not_mutual_google_wikipedia = Diff_list(google_wikipedia_list,wikipedia_google_list)\n",
    "    # keywords that are not mutual from google causes news\n",
    "    not_mutual_wikipedia_google = Diff_list(wikipedia_google_list,google_wikipedia_list)\n",
    "\n",
    "    print(f\"Ammount of successfull granger tests:\")\n",
    "    print(f\"news_granger_causes_google: {len(news_google_list)}\")\n",
    "    print(f\"google_granger_causes_news: {len(google_news_list)}\")\n",
    "    print()\n",
    "    print(f\"news_granger_causes_Wiki: {len(news_wikipedia_list)}\")\n",
    "    print(f\"Wiki_granger_causes_News: {len(wikipedia_news_list)}\")\n",
    "    print()\n",
    "    print(f\"Google_granger_causes_Wikipedia: {len(google_wikipedia_list)}\")\n",
    "    print(f\"Wikipedia_granger_causes_Google: {len(wikipedia_google_list)}\")\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print(f\"Ammount of not mutual granger tests\")\n",
    "    print(f\"Not mutual news granger wikipedia: {len(not_mutual_news_wikipedia)}\")\n",
    "    print(f\"Not mutual wikipedia granger news: {len(not_mutual_wikipedia_news)}\")\n",
    "    print()\n",
    "    print(f\"Not mutual news granger google: {len(not_mutual_news_google)}\")\n",
    "    print(f\"Not mutual google granger news: {len(not_mutual_google_news)}\")\n",
    "    print()\n",
    "    print(f\"Not mutual google granger wikipedia: {len(not_mutual_google_wikipedia)}\")\n",
    "    print(f\"Not mutual wikipedia granger google: {len(not_mutual_wikipedia_google)}\")\n",
    "\n",
    "    data = [[\"news_granger_causes_google\",len(news_granger_causes_google)],\n",
    "            [\"google_granger_causes_news\",len(google_granger_causes_news)],\n",
    "\n",
    "            [\"news_granger_causes_Wiki\",len(news_granger_causes_Wiki)],\n",
    "            [\"Wiki_granger_causes_News\",len(Wiki_granger_causes_News)],\n",
    "\n",
    "            [\"Google_granger_causes_Wikipedia\",len(Google_granger_causes_Wikipedia)],\n",
    "            [\"Wikipedia_granger_causes_Google\",len(Wikipedia_granger_causes_Google)],\n",
    "\n",
    "\n",
    "\n",
    "            [\"not_mutual_news_wikipedia\",len(not_mutual_news_wikipedia)],\n",
    "            [\"not_mutual_wikipedia_news\",len(not_mutual_wikipedia_news)],\n",
    "\n",
    "            [\"not_mutual_news_google\",len(not_mutual_news_google)],\n",
    "            [\"not_mutual_google_news\",len(not_mutual_google_news)],\n",
    "\n",
    "            [\"not_mutual_google_wikipedia\",len(not_mutual_google_wikipedia)],\n",
    "            [\"not_mutual_wikipedia_google\",len(not_mutual_wikipedia_google)]]\n",
    "\n",
    "    return pd.DataFrame(data, columns=['Connection_Granger', media_house])\n",
    "\n",
    "\n",
    "granger_distribution_all = calculate_granger_distribution(real_granger_DataFrame,\"All\")\n",
    "\n",
    "# By media house\n",
    "granger_distribution_Spiegel = calculate_granger_distribution(real_granger_DataFrame_Spiegel,\"Spiegel\")\n",
    "granger_distribution_Bild = calculate_granger_distribution(real_granger_DataFrame_Bild,\"Bild\")\n",
    "granger_distribution_Sueddeutsche = calculate_granger_distribution(real_granger_DataFrame_Sueddeutsche,\"Sueddeutsche\")\n",
    "\n",
    "dfs = [granger_distribution_all,granger_distribution_Spiegel,granger_distribution_Bild,granger_distribution_Sueddeutsche]\n",
    "dfs = [df.set_index('Connection_Granger') for df in dfs]\n",
    "granger_distribution_df = pd.concat(dfs,axis=1)\n",
    "granger_distribution_df.to_csv(\"C:/Users/Jan/Documents/Python_Projects/Bachelorthesis/Bachelorthesis/Analysis/DataFrames/Granger_Distribution_Media_Houses.csv\",index=True)\n",
    "\n",
    "# By season\n",
    "granger_distribution_Spring = calculate_granger_distribution(real_granger_DataFrame_Spring,\"Spring\")\n",
    "granger_distribution_Summer = calculate_granger_distribution(real_granger_DataFrame_Summer,\"Summer\")\n",
    "granger_distribution_Winter = calculate_granger_distribution(real_granger_DataFrame_Winter,\"Winter\")\n",
    "dfs = [granger_distribution_Spring,granger_distribution_Summer,granger_distribution_Winter]\n",
    "dfs = [df.set_index('Connection_Granger') for df in dfs]\n",
    "granger_distribution_df = pd.concat(dfs,axis=1)\n",
    "granger_distribution_df.to_csv(\"C:/Users/Jan/Documents/Python_Projects/Bachelorthesis/Bachelorthesis/Analysis/DataFrames/Granger_Distribution_Season.csv\",index=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perform KPSS and ADF test and perform granger causality test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 135>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    132\u001B[0m     df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(return_list)\n\u001B[0;32m    133\u001B[0m     df \u001B[38;5;241m=\u001B[39m add_lowest_lag_column(df, test_cases)\n\u001B[1;32m--> 135\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_granger\u001B[49m\u001B[43m(\u001B[49m\u001B[43mOccurence_DataFrame\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36mcalculate_granger\u001B[1;34m(dataFrame, fileName)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalculate_granger\u001B[39m(dataFrame,fileName):\n\u001B[1;32m----> 4\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key, item \u001B[38;5;129;01min\u001B[39;00m dataFrame:\n\u001B[0;32m      5\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m      6\u001B[0m             return_dict \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# TODO pass ammount of lags to function and create column names automatically\n",
    "def add_lowest_lag_column(df, testcases):\n",
    "    for testcase in testcases:\n",
    "        test_case_name = list(testcase.items())[0][0]\n",
    "        print(f\"TESTCASE: {test_case_name}\")\n",
    "        wiki_list = df[[f\"ssr_ftest_pValue_lag1_after_diff_{test_case_name}\" ,f'ssr_ftest_pValue_lag2_after_diff_{test_case_name}' , f\"ssr_ftest_pValue_lag3_after_diff_{test_case_name}\" , f\"ssr_ftest_pValue_lag4_after_diff_{test_case_name}\" , f\"ssr_ftest_pValue_lag5_after_diff_{test_case_name}\" , f\"ssr_ftest_pValue_lag6_after_diff_{test_case_name}\" , f\"ssr_ftest_pValue_lag7_after_diff_{test_case_name}\"]] .values.tolist()\n",
    "        result_lst = []\n",
    "        for row in wiki_list:\n",
    "            min_value = min(row)\n",
    "            if min_value <= 0.05:\n",
    "                min_index = row.index(min_value) #not zero based\n",
    "                result_lst.append(min_index+1)\n",
    "            else:\n",
    "                result_lst.append(np.nan)\n",
    "        df[f\"lowest_lag_after_diff_{test_case_name}\"] = result_lst\n",
    "    return df\n",
    "\n",
    "#save dataframe as csv\n",
    "def saveDF_as_CSV(dataframe, filename):\n",
    "    dataframe.to_csv(\"C:/Users/Jan/Documents/Python_Projects/Bachelorthesis/Bachelorthesis/Analysis/DataFrames/\"+ filename +\".csv\",index=False)\n",
    "\n",
    "def calculate_granger(dataFrame,filename):\n",
    "    # strip dataframe to relevant columns\n",
    "    granger_test_df = dataFrame[[\"KeyWord\",\"date\", \"smoothened_normalized_Occurence_in_News\", \"smoothened_normalized_Occurence_in_Wikipedia\",\"smoothened_normalized_Occurence_in_Google\"]]\n",
    "    # set index to date\n",
    "    granger_test_df= granger_test_df.set_index(\"date\")\n",
    "    # add missing dates\n",
    "    #get first and last date\n",
    "    date_range = pd.date_range(granger_test_df.index.min(), granger_test_df.index.max(), freq='D')\n",
    "\n",
    "    # initialize return list\n",
    "    return_list = []\n",
    "\n",
    "\n",
    "    #group dataframe by keyword\n",
    "    grouped_by_keyword = granger_test_df.groupby(\"KeyWord\")\n",
    "\n",
    "    not_testable_keywords = []\n",
    "    # define alpha niveau\n",
    "    alpha_niveau = 0.05\n",
    "\n",
    "\n",
    "    # define number of lags to test\n",
    "    number_of_lags = 7\n",
    "\n",
    "    for key, item in grouped_by_keyword:\n",
    "        try:\n",
    "            return_dict = {}\n",
    "            # get dataframe by keyword\n",
    "            grouped_keyWord = grouped_by_keyword.get_group(key)\n",
    "\n",
    "            #get name of keyword\n",
    "            keyWord = grouped_keyWord[\"KeyWord\"].iloc[0]\n",
    "            return_dict[\"keyword_name\"] = keyWord\n",
    "            print(keyWord)\n",
    "\n",
    "            #add missing dates\n",
    "            grouped_keyWord.index = pd.DatetimeIndex(grouped_keyWord.index)\n",
    "            grouped_keyWord = grouped_keyWord.reindex(date_range, fill_value=0)\n",
    "\n",
    "            for i in range(2):\n",
    "                if i == 0:\n",
    "                    stationary_information = \"before_diff\"\n",
    "                    current_test_column_Wiki = \"smoothened_normalized_Occurence_in_Wikipedia\"\n",
    "                    current_test_column_News = \"smoothened_normalized_Occurence_in_News\"\n",
    "                    current_test_column_Google = \"smoothened_normalized_Occurence_in_Google\"\n",
    "\n",
    "                    News_grangerCauses_Wikipedia = [current_test_column_Wiki, current_test_column_News]\n",
    "                    Wikipedia_grangerCauses_News = [current_test_column_News, current_test_column_Wiki]\n",
    "\n",
    "                    News_grangerCauses_Google = [current_test_column_Google, current_test_column_News]\n",
    "                    Google_grangerCauses_News = [current_test_column_News, current_test_column_Google]\n",
    "\n",
    "                    Google_grangerCauses_Wikipedia = [current_test_column_Wiki, current_test_column_Google]\n",
    "                    Wikipedia_grangerCauses_Google = [current_test_column_Google, current_test_column_Wiki]\n",
    "\n",
    "                # make data stationary\n",
    "                if i == 1:\n",
    "                    current_test_column_Wiki = \"smoothened_normalized_Occurence_in_Wikipedia_diff\"\n",
    "                    current_test_column_News = \"smoothened_normalized_Occurence_in_News_diff\"\n",
    "                    current_test_column_Google = \"smoothened_normalized_Occurence_in_Google_diff\"\n",
    "                    grouped_keyWord[\"smoothened_normalized_Occurence_in_Wikipedia_diff\"] = grouped_keyWord['smoothened_normalized_Occurence_in_Wikipedia'].diff()\n",
    "                    grouped_keyWord[\"smoothened_normalized_Occurence_in_News_diff\"] = grouped_keyWord['smoothened_normalized_Occurence_in_News'].diff()\n",
    "                    grouped_keyWord[\"smoothened_normalized_Occurence_in_Google_diff\"] = grouped_keyWord['smoothened_normalized_Occurence_in_Google'].diff()\n",
    "\n",
    "\n",
    "                    grouped_keyWord.dropna(subset = [\"smoothened_normalized_Occurence_in_Wikipedia_diff\"], inplace=True)\n",
    "                    grouped_keyWord.dropna(subset = [\"smoothened_normalized_Occurence_in_News_diff\"], inplace=True)\n",
    "                    grouped_keyWord.dropna(subset = [\"smoothened_normalized_Occurence_in_Google_diff\"], inplace=True)\n",
    "                    stationary_information = \"after_diff\"\n",
    "\n",
    "                    News_grangerCauses_Wikipedia = [current_test_column_Wiki, current_test_column_News]\n",
    "                    Wikipedia_grangerCauses_News = [current_test_column_News, current_test_column_Wiki]\n",
    "\n",
    "                    News_grangerCauses_Google = [current_test_column_Google, current_test_column_News]\n",
    "                    Google_grangerCauses_News = [current_test_column_News, current_test_column_Google]\n",
    "\n",
    "                    Google_grangerCauses_Wikipedia = [current_test_column_Wiki, current_test_column_Google]\n",
    "                    Wikipedia_grangerCauses_Google = [current_test_column_Google, current_test_column_Wiki]\n",
    "\n",
    "\n",
    "                    # prepare for grangercausalitytest -> will be after diff\n",
    "                    test_cases = []\n",
    "                    News_grangerCauses_Wikipedia_dict ={\"News_grangerCauses_Wikipedia\" : News_grangerCauses_Wikipedia}\n",
    "                    Wikipedia_grangerCauses_News_dict ={\"Wikipedia_grangerCauses_News\" : Wikipedia_grangerCauses_News}\n",
    "\n",
    "                    News_grangerCauses_Google_dict ={\"News_grangerCauses_Google\" : News_grangerCauses_Google}\n",
    "                    Google_grangerCauses_News_dict ={\"Google_grangerCauses_News\" : Google_grangerCauses_News}\n",
    "\n",
    "                    Google_grangerCauses_Wiki_dict ={\"Google_grangerCauses_Wikipedia\" : Google_grangerCauses_Wikipedia}\n",
    "                    Wikipedia_grangerCauses_Google_dict ={\"Wikipedia_grangerCauses_Google\" : Wikipedia_grangerCauses_Google}\n",
    "\n",
    "\n",
    "                    test_cases.append(News_grangerCauses_Wikipedia_dict)\n",
    "                    test_cases.append(Wikipedia_grangerCauses_News_dict)\n",
    "\n",
    "                    test_cases.append(News_grangerCauses_Google_dict)\n",
    "                    test_cases.append(Google_grangerCauses_News_dict)\n",
    "\n",
    "                    test_cases.append(Google_grangerCauses_Wiki_dict)\n",
    "                    test_cases.append(Wikipedia_grangerCauses_Google_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #ADF test\n",
    "                #print(f'ADF Test: {keyword_name} time series')\n",
    "                adf_test_result_wikipedia = adfuller(pd.DataFrame(grouped_keyWord[current_test_column_Wiki]).values)\n",
    "                adf_test_result_news      = adfuller(pd.DataFrame(grouped_keyWord[current_test_column_News]).values)\n",
    "                adf_test_result_google    = adfuller(pd.DataFrame(grouped_keyWord[current_test_column_Google]).values)\n",
    "\n",
    "\n",
    "\n",
    "                # save ADF test results\n",
    "                # test for stationary\n",
    "                # wikipedia\n",
    "                ADF_p_value_wiki = adf_test_result_wikipedia[1]\n",
    "                ADF_p_value_news = adf_test_result_news[1]\n",
    "                ADF_p_value_google = adf_test_result_google[1]\n",
    "\n",
    "                # wiki\n",
    "                return_dict[f\"ADF_Statistics_wiki_{stationary_information}\"] = adf_test_result_wikipedia[0]\n",
    "                return_dict[f\"ADF_p_value_wiki_{stationary_information}\"] = ADF_p_value_wiki\n",
    "                return_dict[f\"ADF_critical_values_wiki_{stationary_information}\"] = adf_test_result_wikipedia[4]\n",
    "                # news\n",
    "                return_dict[f\"ADF_Statistics_news_{stationary_information}\"] = adf_test_result_news[0]\n",
    "                return_dict[f\"ADF_p_value_news_{stationary_information}\"] = ADF_p_value_wiki\n",
    "                return_dict[f\"ADF_critical_values_news_{stationary_information}\"] = adf_test_result_news[4]\n",
    "                # google\n",
    "                return_dict[f\"ADF_Statistics_news_{stationary_information}\"] = adf_test_result_google[0]\n",
    "                return_dict[f\"ADF_p_value_news_{stationary_information}\"] = ADF_p_value_google\n",
    "                return_dict[f\"ADF_critical_values_news_{stationary_information}\"] = adf_test_result_google[4]\n",
    "\n",
    "                # KPSS Test\n",
    "                # test for non seasonary or trend\n",
    "                kpss_statistic_wiki, kpss_pValue_wiki, kpss_nLags_wiki, kpss_criticalValues_wiki = kpss(pd.DataFrame(grouped_keyWord[current_test_column_Wiki]).values)\n",
    "                kpss_statistic_news, kpss_pValue_news, kpss_nLags_news, kpss_criticalValues_news = kpss(pd.DataFrame(grouped_keyWord[current_test_column_News]).values)\n",
    "                kpss_statistic_google, kpss_pValue_google, kpss_nLags_google, kpss_criticalValues_google = kpss(pd.DataFrame(grouped_keyWord[current_test_column_Google]).values)\n",
    "\n",
    "\n",
    "\n",
    "                #Check for stationary\n",
    "                ADF_stationary_wiki = False\n",
    "                ADF_stationary_news = False\n",
    "                ADF_stationary_google = False\n",
    "                KPSS_stationary_wiki = False\n",
    "                KPSS_stationary_news = False\n",
    "                KPSS_stationary_google =False\n",
    "\n",
    "\n",
    "                if ADF_p_value_wiki <= alpha_niveau:\n",
    "                    ADF_stationary_wiki = True\n",
    "\n",
    "                if ADF_p_value_news <= alpha_niveau:\n",
    "                    ADF_stationary_news = True\n",
    "\n",
    "                if ADF_p_value_google <= alpha_niveau:\n",
    "                    ADF_stationary_google = True\n",
    "\n",
    "                if kpss_pValue_wiki >= alpha_niveau:\n",
    "                    KPSS_stationary_wiki = True\n",
    "\n",
    "                if kpss_pValue_news >= alpha_niveau:\n",
    "                    KPSS_stationary_news = True\n",
    "\n",
    "                if kpss_pValue_google >= alpha_niveau:\n",
    "                    KPSS_stationary_google = True\n",
    "                # save results\n",
    "                return_dict[f\"ADF_stationary_wiki_{stationary_information}\"] = ADF_stationary_wiki\n",
    "                return_dict[f\"ADF_stationary_news_{stationary_information}\"] = ADF_stationary_news\n",
    "                return_dict[f\"ADF_stationary_google_{stationary_information}\"] = ADF_stationary_google\n",
    "                return_dict[f\"KPSS_stationary_wiki_{stationary_information}\"] = KPSS_stationary_wiki\n",
    "                return_dict[f\"KPSS_stationary_news_{stationary_information}\"] = KPSS_stationary_news\n",
    "                return_dict[f\"KPSS_stationary_google_{stationary_information}\"] = KPSS_stationary_google\n",
    "\n",
    "\n",
    "            for test_case in test_cases:\n",
    "\n",
    "                # perform grangercausalitytest\n",
    "                granger_test_result = grangercausalitytests(grouped_keyWord[list(test_case.values())[0]], maxlag=number_of_lags)\n",
    "                test_case_name = list(test_case.items())[0][0]\n",
    "\n",
    "\n",
    "                test_name='ssr_ftest'\n",
    "                #column of result\n",
    "                column = 1\n",
    "\n",
    "                p_values = [round(granger_test_result[i+1][0][test_name][column],4) for i in range(number_of_lags)]\n",
    "                #print(p_values)\n",
    "                for lag in range(number_of_lags):\n",
    "                    # lags are not zero based\n",
    "                    lag_not_zero_based = lag+1\n",
    "\n",
    "                    if column == 1:\n",
    "                        column = \"pValue\"\n",
    "                    key = f\"{test_name}_{column}_lag{lag_not_zero_based}_{stationary_information}_{test_case_name}\"\n",
    "                    value = p_values[lag]\n",
    "                    return_dict[key] = value\n",
    "            return_list.append(return_dict)\n",
    "\n",
    "        except Exception:\n",
    "            print(f\"Error! {keyWord}\" )\n",
    "            print(Exception.args)\n",
    "            not_testable_keywords.append(keyWord)\n",
    "            pass\n",
    "    df = pd.DataFrame(return_list)\n",
    "    df = add_lowest_lag_column(df, test_cases)\n",
    "\n",
    "    saveDF_as_CSV(df, f\"Granger_DataFrame{filename}\")\n",
    "\n",
    "calculate_granger(Occurence_DataFrame,\"\")\n",
    "calculate_granger(Occurence_DataFrame_Bild,\"_Bild\")\n",
    "calculate_granger(Occurence_DataFrame_Spiegel,\"_Spiegel\")\n",
    "calculate_granger(Occurence_DataFrame_Sueddeutsche,\"Sueddeutsche\")\n",
    "\n",
    "calculate_granger(Occurence_DataFrame_Spring,\"_Spring\")\n",
    "calculate_granger(Occurence_DataFrame_Summer,\"_Summer\")\n",
    "calculate_granger(Occurence_DataFrame_Winter,\"_Winter\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}