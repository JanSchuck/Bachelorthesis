{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ea68f1",
   "metadata": {},
   "source": [
    "# Analysing german news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0155c4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f7377d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is an open source data analysis and manipulation tool\n",
    "import pandas as pd\n",
    "from pandas import json_normalize \n",
    "\n",
    "# os gives access to the operating system\n",
    "import os\n",
    "# The datetime module supplies classes for manipulating dates and times.\n",
    "from datetime import datetime \n",
    "import datetime\n",
    "# This module provides various time-related functions.\n",
    "import time\n",
    "\n",
    "# Natural language toolkit\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# library to create visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "\n",
    "\n",
    "# open source library for automating downloading of reports from Google Trends\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "# library to get html of website (wikipedia)\n",
    "import requests\n",
    "# json to use wikipedia return\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abf3d7",
   "metadata": {},
   "source": [
    "## Load data into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c9919b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29436 entries, 0 to 70452\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Titel                  29436 non-null  object \n",
      " 1   Date_Info              29436 non-null  object \n",
      " 2   URL                    29436 non-null  object \n",
      " 3   Overline               20256 non-null  object \n",
      " 4   Breadcrumb             14851 non-null  object \n",
      " 5   author                 3643 non-null   object \n",
      " 6   detailed_informations  5148 non-null   object \n",
      " 7   Zugriff_Datum          20872 non-null  float64\n",
      " 8   News_page              29436 non-null  object \n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Folderpath to CSV's\n",
    "CSV_folderPath=\"/Users/jan/Documents/Python_Projects/Bachelorthesis/CSV_Backlog\"\n",
    "\n",
    "# Initialize list<csv> for Spiegel, Sueddeutsche, Bild and combined \n",
    "Spiegel_csvs = []\n",
    "Sueddeutsche_csvs = []\n",
    "Bild_csvs = []\n",
    "Combi_csvs = []\n",
    "\n",
    "\n",
    "# Change directory to CSV_folderPath\n",
    "os.chdir(CSV_folderPath)\n",
    "\n",
    "# iterate over all files\n",
    "for file in os.listdir():\n",
    "    # that are csvs...\n",
    "    if file.endswith(\".csv\"):        \n",
    "        # read csv to dataframe\n",
    "        df =pd.read_csv(file, index_col=None, header=0)\n",
    "        # all df append to combi...\n",
    "        Combi_csvs.append(df)\n",
    "        # sort dataframe to its list\n",
    "        #Spiegel\n",
    "        if file.startswith(\"Spiegel\"):\n",
    "            Spiegel_csvs.append(df)\n",
    "        # Sueddeutsche\n",
    "        elif file.startswith(\"Sueddeutsche\"):\n",
    "            Sueddeutsche_csvs.append(df)\n",
    "        # Bild\n",
    "        elif file.startswith(\"Bild\"):\n",
    "            Bild_csvs.append(df)\n",
    "    \n",
    "All_articles = pd.concat(Combi_csvs, axis=0, ignore_index=True)\n",
    "Spiegel_articles = pd.concat(Spiegel_csvs, axis=0, ignore_index=True)\n",
    "Sueddeutsche_articles = pd.concat(Sueddeutsche_csvs, axis=0, ignore_index=True)\n",
    "if(len(Bild_csvs)) != 0:\n",
    "    Bild_articles = pd.concat(Bild_csvs, axis=0, ignore_index=True)\n",
    "else:\n",
    "    Bild_articles=[]\n",
    "\n",
    "    \n",
    "# Drop duplicates\n",
    "\n",
    "# All articles\n",
    "All_articles= All_articles.drop_duplicates(subset=\"Titel\")\n",
    "\n",
    "# Spiegel articles\n",
    "Spiegel_articles = Spiegel_articles.drop_duplicates(subset=\"Titel\")\n",
    "\n",
    "# Sueddeutsch articles\n",
    "Sueddeutsche_articles = Sueddeutsche_articles.drop_duplicates(subset=\"Titel\")\n",
    "\n",
    "# Bild articles\n",
    "#Bild_articles = Bild_articles.drop_duplicates(subset=\"title\")\n",
    "All_articles.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051ceb3",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96245ac",
   "metadata": {},
   "source": [
    "### Cast Date_Info to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dbf25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All articles\n",
    "All_articles[\"Date_Info\"] = All_articles[\"Date_Info\"].str.strip()\n",
    "All_articles[\"Date_Info\"] = pd.to_datetime(All_articles[\"Date_Info\"], format='%d%m%Y_%H:%M')\n",
    "# Spiegel articles\n",
    "Spiegel_articles[\"Date_Info\"] = Spiegel_articles[\"Date_Info\"].str.strip()\n",
    "Spiegel_articles[\"Date_Info\"] = pd.to_datetime(Spiegel_articles[\"Date_Info\"], format='%d%m%Y_%H:%M')\n",
    "# Sueddeutsche articles\n",
    "Sueddeutsche_articles[\"Date_Info\"] = Sueddeutsche_articles[\"Date_Info\"].str.strip()\n",
    "Sueddeutsche_articles[\"Date_Info\"] = pd.to_datetime(Sueddeutsche_articles[\"Date_Info\"], format='%d%m%Y_%H:%M')\n",
    "# Bild articles\n",
    "#Bild_carticles[\"Date_Info\"] = pd.to_datetime(Bild_carticles[\"Date_Info\"], format='%d%m%Y_%H:%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95f57d",
   "metadata": {},
   "source": [
    "## Initial data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c10238",
   "metadata": {},
   "source": [
    "### Ammount of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a18b9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spiegel articles: 8564\n",
      "Sueddeutsche articles: 20873\n",
      "Bild articles: 0\n",
      "Overall: 29436\n"
     ]
    }
   ],
   "source": [
    "# Spiegel articles\n",
    "ammount_of_Spiegel_articles = len(Spiegel_articles)\n",
    "print(f\"Spiegel articles: {ammount_of_Spiegel_articles}\")\n",
    "\n",
    "# Sueddeutsche articles\n",
    "ammount_of_Sueddeutsche_articles = len(Sueddeutsche_articles)\n",
    "print(f\"Sueddeutsche articles: {ammount_of_Sueddeutsche_articles}\")\n",
    "\n",
    "# Bild articles\n",
    "ammount_of_Bild_articles = len(Bild_articles)\n",
    "print(f\"Bild articles: {ammount_of_Bild_articles}\")\n",
    "\n",
    "# All articles\n",
    "ammount_of_Combined_articles = len(All_articles)\n",
    "print(f\"Overall: {ammount_of_Combined_articles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8fe3a8",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004b1774",
   "metadata": {},
   "source": [
    "### Exploding titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b13b41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_titles(title_column):\n",
    "    title_column =title_column.to_frame()\n",
    "    tokenized_titles=[]\n",
    "    title_column[\"tokenized_title\"] = \"\"\n",
    "    for index, row in title_column.iterrows():\n",
    "        # tokenize\n",
    "        tokenized_title = word_tokenize(row.Titel)\n",
    "        \n",
    "        # remove stopwords & numbers/punction\n",
    "        tokenized_title = [word for word in tokenized_title if word not in stopwords.words(\"german\")]\n",
    "        \n",
    "        # add to row\n",
    "        row.tokenized_title = tokenized_title\n",
    "        \n",
    "    return title_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "93859d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(data, chunk_size): \n",
    "    while data:\n",
    "        chunk, data = data[:chunk_size], data[chunk_size:]\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28574384",
   "metadata": {},
   "source": [
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "482da2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_interest_over_time(keyword_list):\n",
    "    # connect to google\n",
    "    pytrends = TrendReq(hl='de', tz='60') \n",
    "\n",
    "    # keywords\n",
    "    keywords = keyword_list\n",
    "\n",
    "    # build payload\n",
    "    startDate = \"2021-11-07\"\n",
    "    dateTime=datetime.datetime.now()\n",
    "    currentDate = dateTime.strftime(\"%Y-%m-%d\")\n",
    "    timeframe = startDate +\" \"+ str(currentDate)\n",
    "    dataframes = []\n",
    "    for keyword in keywords:\n",
    "        keyword_list = []\n",
    "        keyword_list.append(keyword)\n",
    "        pytrends.build_payload(keyword_list, cat=0, timeframe= str(timeframe) ) \n",
    "        \n",
    "        # get data with interest over time\n",
    "        data = pytrends.interest_over_time() \n",
    "        dataframes.append(data)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # concat all google trends results\n",
    "    dfs = pd.concat(dataframes, axis=1)\n",
    "    # remove deprecated column\n",
    "    dfs=dfs.loc[:,~dfs.columns.str.startswith('isPartial')]\n",
    "\n",
    "    # get df to right format\n",
    "    google_df = dfs.unstack().reset_index() \n",
    "    google_df = google_df.rename(columns={\"level_0\" : \"KeyWord\", 0 : \"Occurence_in_Google\"})\n",
    "    \n",
    "    #fill all NaN with 0\n",
    "    google_df[\"Occurence_in_Google\"] = google_df[\"Occurence_in_Google\"].fillna(0)\n",
    "    google_df[\"Occurence_in_Google\"] = google_df[\"Occurence_in_Google\"].astype(int)\n",
    "    \n",
    "    google_df[\"Occurence_in_Google_normalized\"] = google_df[\"Occurence_in_Google\"].div(100)\n",
    "    return google_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50344ae7",
   "metadata": {},
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "d9897c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_interest_over_time(keyword_list):\n",
    "    dateTime=datetime.datetime.now()\n",
    "    currentDate = dateTime.strftime(\"%Y%m%d\")\n",
    "    return_list=[]\n",
    "    # chunk keywords due to wikimedia policy\n",
    "    for chunk in make_chunks(keyword_list, 200):        \n",
    "        for keyword in chunk:\n",
    "            headers = {'user-agent': 'Influence_of_daily_political_news_on_the_use_of_wikipedia/1.0 (jan.schuckatt-online.de)'}\n",
    "            url = f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/de.wikipedia.org/all-access/all-agents/{keyword}/daily/20211107/{currentDate}\"\n",
    "            r = requests.get(url, headers=headers)\n",
    "            return_list.append(r.text)\n",
    "        time.sleep(5)\n",
    "    # deserialize json   \n",
    "    json_dict=[]\n",
    "    print(return_list)\n",
    "    return\n",
    "    for wiki_dict in return_list:\n",
    "        data = json.loads(wiki_dict)\n",
    "        json_dict.append(data)\n",
    "        \n",
    "    #json to dataframe\n",
    "    lst_of_df=[]\n",
    "    for counter in range(len(json_dict)):\n",
    "        try:\n",
    "            lst_of_df.append(pd.DataFrame.from_records(json_dict[counter][\"items\"]))\n",
    "        except:\n",
    "            print(json_dict[counter])\n",
    "    return_DF = pd.concat(good_dfs)\n",
    "    return_DF = return_DF.rename(columns={'article': 'keyWord', 'timestamp': 'date',\"views\" : \"Occurence in Wikipedia\"})\n",
    "    return_DF = return_DF.drop(['project', 'granularity',\"access\",\"agent\"], axis=1, inplace=True)\n",
    "    return pd.concat(good_dfs)\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5ceed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "8e8ef273",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-343-4818fd5bdf8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_wikiData_jsons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wikipedia_interst_over_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-331-63b90f841826>\u001b[0m in \u001b[0;36mget_wikipedia_interst_over_time\u001b[0;34m(keyword_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mjson_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwiki_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_wikiData_jsons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mjson_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "raw_wikiData_jsons = get_wikipedia_interst_over_time(keyword_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb14f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_wikiData_jsons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a857b49",
   "metadata": {},
   "source": [
    "### Titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "170fd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurence_of_all_titles(data_frame, columnName):\n",
    "    # Group titles by columnName\n",
    "    all_titles = data_frame.groupby(columnName).size()\n",
    "    \n",
    "    #sort titles\n",
    "    all_titles = all_titles.sort_values(ascending = False)\n",
    "    \n",
    "    return all_titles\n",
    "\n",
    "def get_occurence_of_all_capital_titles(data_frame, columnName):\n",
    "    all_titles=data_frame.groupby(columnName).size()\n",
    "\n",
    "    capital_titles=[]\n",
    "    counter=0\n",
    "    for title in all_titles.items():\n",
    "        title_s=str(title)\n",
    "        title_s=title_s.strip()\n",
    "        if title_s.istitle():\n",
    "            capital_titles.append(title)        \n",
    "    Titles = pd.DataFrame.from_records(\n",
    "    capital_titles, columns=['Title','Occurence'])\n",
    "    return Titles.sort_values(by=['Occurence'],ascending=False)\n",
    "\n",
    "def get_titles_with_minimum_occurence_N(data_frame, N):\n",
    "    all_Titles = get_occurence_of_all_capital_titles(data_frame, \"tokenized_titles\")\n",
    "    above_N = []\n",
    "    for index,row in all_Titles.iterrows():\n",
    "        if int(row.Occurence) >= N:\n",
    "            above_N.append(row)\n",
    "            \n",
    "    above_N = pd.DataFrame(above_N, columns=['Title', 'Occurence'])\n",
    "\n",
    "    return data_frame[data_frame[\"tokenized_titles\"].isin(above_N.Title)]\n",
    "\n",
    "def get_titles_with_minimum_occurence_N(data_frame, N):\n",
    "    all_Titles = get_occurence_of_all_capital_titles(data_frame, \"tokenized_titles\")\n",
    "    above_N = []\n",
    "    for index,row in all_Titles.iterrows():\n",
    "        if int(row.Occurence) >= N:\n",
    "            above_N.append(row)\n",
    "            \n",
    "    above_N = pd.DataFrame(above_N, columns=['Title', 'Occurence'])\n",
    "\n",
    "    return data_frame[data_frame[\"tokenized_titles\"].isin(above_N.Title)]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c67e0",
   "metadata": {},
   "source": [
    "# Build big Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e840f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_articles[\"tokenized_titles\"] = explode_titles(All_articles[\"Titel\"]).tokenized_title\n",
    "All_articles = All_articles.explode(\"tokenized_titles\")\n",
    "uninteresting_titles= [\"Der\", \"Die\", \"Das\",\"Was\",\"Warum\",\"Er\",\"Sie\", \"Es\", \"Ich\", \"Du\", \"Mit\",\"Wie\",\"Ein\",\"So\",\"Wir\",\n",
    "                       \":\",\"»\",\"«\", \",\" ,\"\\'\\'\",\"``\", \"_\",\"-\" \".\",\"?\",\"–\", \"-\",\".\",\"!\"]\n",
    "\n",
    "All_articles=All_articles[~All_articles.tokenized_titles.isin(uninteresting_titles)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64812edc",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4ac2b",
   "metadata": {},
   "source": [
    "### Ammount of articles with occurence above n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3bec31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n:10 = 1811\n",
      "n:25 = 584\n",
      "n:50 = 239\n",
      "n:75 = 125\n",
      "n:100 = 72\n",
      "n:125 = 48\n",
      "n:150 = 35\n",
      "n:200 = 21\n"
     ]
    }
   ],
   "source": [
    "occurence = [10,25,50,75,100,125,150,200]\n",
    "for n in occurence:\n",
    "    x = get_titles_with_minimum_occurence_N(All_articles, n)\n",
    "    y = get_occurence_of_all_capital_titles(x,\"tokenized_titles\")\n",
    "    print(\"n:\"+str(n) +\" = \"+str(len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fd45b",
   "metadata": {},
   "source": [
    "# Build Occurence DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "141a77cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Occurence_df(dataframe, n_occurences):\n",
    "    article_over_occurence = get_titles_with_minimum_occurence_N(dataframe[[\"Date_Info\", \"tokenized_titles\"]], n_occurences)\n",
    "\n",
    "    # prep keyword list \"Titles\"\n",
    "    keyword_list = list(article_over_occurence[\"tokenized_titles\"].drop_duplicates())\n",
    "    # get google data\n",
    "    google_data = get_google_interest_over_time(keyword_list)\n",
    "    # get wikipedia data\n",
    "    wikipedia_data = get_wikipedia_interest_over_time(keyword_list)\n",
    "\n",
    "    #rename columns\n",
    "    article_over_occurence = article_over_occurence.rename(columns={\"Date_Info\" : \"date\", \"tokenized_titles\" : \"Occurence_in_news\"})\n",
    "    #convert to datetime\n",
    "    article_over_occurence['date'] = pd.to_datetime(article_over_occurence.date) \n",
    "    #convert datetime format to googles datetime format\n",
    "    article_over_occurence['date'] = article_over_occurence['date'].dt.strftime('%Y-%m-%d')\n",
    "    #reshape our data frame to look like google data frame\n",
    "    occurence_df=article_over_occurence.pivot_table(index='date', columns='Occurence_in_news', aggfunc='size') \\\n",
    "                .rename_axis(None, axis=1)\n",
    "    #fill all NaN with 0\n",
    "    occurence_df = occurence_df.fillna(0)\n",
    "    occurence_df = occurence_df.astype(int)\n",
    "\n",
    "    # bring news df to right format\n",
    "    news_df = occurence_df.unstack().reset_index()\n",
    "    news_df = news_df.rename(columns={\"level_0\" : \"KeyWord\", 0 : \"Occurence_in_News\"})\n",
    "    news_df[\"date\"]=pd.to_datetime(news_df[\"date\"], format='%Y-%m-%d')\n",
    "    \n",
    "    # normalize Occurence_in_News\n",
    "    \n",
    "    # merge google and news\n",
    "    full_df = pd.merge(news_df, google_data, on=['KeyWord',\"date\"], how='outer')\n",
    "    #fill all NaN with 0\n",
    "    full_df[\"Occurence_in_Google\"] = full_df[\"Occurence_in_Google\"].fillna(0)\n",
    "    full_df[\"Occurence_in_Google\"] = full_df[\"Occurence_in_Google\"].astype(int)\n",
    "    data_frame_list=[]\n",
    "    for keyword in keyword_list:\n",
    "        keyworded_dataFrame = full_df.loc[full_df[\"KeyWord\"] == keyword]\n",
    "\n",
    "        df_chunk = keyworded_dataFrame\n",
    "        df_chunk[\"Occurence_in_News_Normalized\"]=(keyworded_dataFrame[\"Occurence_in_News\"]-keyworded_dataFrame[\"Occurence_in_News\"].min())/(keyworded_dataFrame[\"Occurence_in_News\"].max()-keyworded_dataFrame[\"Occurence_in_News\"].min())\n",
    "    \n",
    "        data_frame_list.append(df_chunk)\n",
    "    merged = pd.concat(data_frame_list)\n",
    "    return merged\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "600cc146",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='trends.google.com', port=443): Read timed out. (read timeout=5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mtimeout\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    756\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             raise ReadTimeoutError(\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Read timed out. (read timeout=%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='trends.google.com', port=443): Read timed out. (read timeout=5)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-9c1752df4c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_Occurence_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAll_articles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-87267f234622>\u001b[0m in \u001b[0;36mbuild_Occurence_df\u001b[0;34m(dataframe, n_occurences)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mkeyword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_over_occurence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokenized_titles\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgoogle_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_google_interest_over_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#rename columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-23f902a478e5>\u001b[0m in \u001b[0;36mget_google_interest_over_time\u001b[0;34m(keyword_list)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# get data with interest over time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterest_over_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mdataframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytrends/request.py\u001b[0m in \u001b[0;36minterest_over_time\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# make the request and parse the returned json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         req_json = self._get_data(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrendReq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTEREST_OVER_TIME_URL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrendReq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_METHOD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pytrends/request.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self, url, method, trim_chars, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m                               cookies=self.cookies, **kwargs, **self.requests_args)  # DO NOT USE retries or backoff_factor here\n\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             response = s.get(url, timeout=self.timeout, cookies=self.cookies,\n\u001b[0m\u001b[1;32m    127\u001b[0m                              **kwargs, **self.requests_args)   # DO NOT USE retries or backoff_factor here\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# check if the response contains json and throw an exception otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='trends.google.com', port=443): Read timed out. (read timeout=5)"
     ]
    }
   ],
   "source": [
    "build_Occurence_df(All_articles,50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67427bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "efef2126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>article</th>\n",
       "      <th>granularity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>access</th>\n",
       "      <th>agent</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Mann</td>\n",
       "      <td>daily</td>\n",
       "      <td>2021110700</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Mann</td>\n",
       "      <td>daily</td>\n",
       "      <td>2021110800</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Mann</td>\n",
       "      <td>daily</td>\n",
       "      <td>2021110900</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Mann</td>\n",
       "      <td>daily</td>\n",
       "      <td>2021111000</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Mann</td>\n",
       "      <td>daily</td>\n",
       "      <td>2021111100</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Neue</td>\n",
       "      <td>daily</td>\n",
       "      <td>2022012000</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Neue</td>\n",
       "      <td>daily</td>\n",
       "      <td>2022012100</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Neue</td>\n",
       "      <td>daily</td>\n",
       "      <td>2022012200</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Neue</td>\n",
       "      <td>daily</td>\n",
       "      <td>2022012300</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Neue</td>\n",
       "      <td>daily</td>\n",
       "      <td>2022012400</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1577 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         project article granularity   timestamp      access       agent  \\\n",
       "0   de.wikipedia    Mann       daily  2021110700  all-access  all-agents   \n",
       "1   de.wikipedia    Mann       daily  2021110800  all-access  all-agents   \n",
       "2   de.wikipedia    Mann       daily  2021110900  all-access  all-agents   \n",
       "3   de.wikipedia    Mann       daily  2021111000  all-access  all-agents   \n",
       "4   de.wikipedia    Mann       daily  2021111100  all-access  all-agents   \n",
       "..           ...     ...         ...         ...         ...         ...   \n",
       "74  de.wikipedia    Neue       daily  2022012000  all-access  all-agents   \n",
       "75  de.wikipedia    Neue       daily  2022012100  all-access  all-agents   \n",
       "76  de.wikipedia    Neue       daily  2022012200  all-access  all-agents   \n",
       "77  de.wikipedia    Neue       daily  2022012300  all-access  all-agents   \n",
       "78  de.wikipedia    Neue       daily  2022012400  all-access  all-agents   \n",
       "\n",
       "    views  \n",
       "0     174  \n",
       "1     157  \n",
       "2     140  \n",
       "3     178  \n",
       "4     181  \n",
       "..    ...  \n",
       "74     10  \n",
       "75      5  \n",
       "76      9  \n",
       "77     11  \n",
       "78     13  \n",
       "\n",
       "[1577 rows x 7 columns]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_wikiData_jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "59f30d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>article</th>\n",
       "      <th>granularity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>access</th>\n",
       "      <th>agent</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Mann</td>\n",
       "      <td>daily</td>\n",
       "      <td>2021110700</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Mann</td>\n",
       "      <td>daily</td>\n",
       "      <td>2021110800</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Mann</td>\n",
       "      <td>daily</td>\n",
       "      <td>2021110900</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Mann</td>\n",
       "      <td>daily</td>\n",
       "      <td>2021111000</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Mann</td>\n",
       "      <td>daily</td>\n",
       "      <td>2021111100</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Neue</td>\n",
       "      <td>daily</td>\n",
       "      <td>2022012000</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Neue</td>\n",
       "      <td>daily</td>\n",
       "      <td>2022012100</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Neue</td>\n",
       "      <td>daily</td>\n",
       "      <td>2022012200</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Neue</td>\n",
       "      <td>daily</td>\n",
       "      <td>2022012300</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>de.wikipedia</td>\n",
       "      <td>Neue</td>\n",
       "      <td>daily</td>\n",
       "      <td>2022012400</td>\n",
       "      <td>all-access</td>\n",
       "      <td>all-agents</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1577 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         project article granularity   timestamp      access       agent  \\\n",
       "0   de.wikipedia    Mann       daily  2021110700  all-access  all-agents   \n",
       "1   de.wikipedia    Mann       daily  2021110800  all-access  all-agents   \n",
       "2   de.wikipedia    Mann       daily  2021110900  all-access  all-agents   \n",
       "3   de.wikipedia    Mann       daily  2021111000  all-access  all-agents   \n",
       "4   de.wikipedia    Mann       daily  2021111100  all-access  all-agents   \n",
       "..           ...     ...         ...         ...         ...         ...   \n",
       "74  de.wikipedia    Neue       daily  2022012000  all-access  all-agents   \n",
       "75  de.wikipedia    Neue       daily  2022012100  all-access  all-agents   \n",
       "76  de.wikipedia    Neue       daily  2022012200  all-access  all-agents   \n",
       "77  de.wikipedia    Neue       daily  2022012300  all-access  all-agents   \n",
       "78  de.wikipedia    Neue       daily  2022012400  all-access  all-agents   \n",
       "\n",
       "    views  \n",
       "0     174  \n",
       "1     157  \n",
       "2     140  \n",
       "3     178  \n",
       "4     181  \n",
       "..    ...  \n",
       "74     10  \n",
       "75      5  \n",
       "76      9  \n",
       "77     11  \n",
       "78     13  \n",
       "\n",
       "[1577 rows x 7 columns]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9733a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEWSPAGE\n",
    "# get all titles that occure above N\n",
    "article_over_occurence = get_titles_with_minimum_occurence_N(All_articles[[\"Date_Info\", \"tokenized_titles\"]], 50)\n",
    "\n",
    "# prep keyword list \"Titles\"\n",
    "keyword_list = list(article_over_occurence[\"tokenized_titles\"].drop_duplicates())\n",
    "\n",
    "#rename columns\n",
    "article_over_occurence = article_over_occurence.rename(columns={\"Date_Info\" : \"date\", \"tokenized_titles\" : \"Occurence_in_news\"})\n",
    "#convert to datetime\n",
    "article_over_occurence['date'] = pd.to_datetime(article_over_occurence.date) \n",
    "#convert datetime format to googles datetime format\n",
    "article_over_occurence['date'] = article_over_occurence['date'].dt.strftime('%Y-%m-%d')\n",
    "#reshape our data frame to look like google data frame\n",
    "occurence_df=article_over_occurence.pivot_table(index='date', columns='Occurence_in_news', aggfunc='size') \\\n",
    "            .rename_axis(None, axis=1)\n",
    "#fill all NaN with 0\n",
    "occurence_df = occurence_df.fillna(0)\n",
    "occurence_df = occurence_df.astype(int)\n",
    "\n",
    "# GOOGLE \n",
    "# google trends laden\n",
    "data = google_interest_over_time(keyword_list)\n",
    "# concat all google trends results\n",
    "dfs = pd.concat(data, axis=1)\n",
    "# remove deprecated column\n",
    "dfs=dfs.loc[:,~dfs.columns.str.startswith('isPartial')]\n",
    "\n",
    "\n",
    "# get current datetime for later filenames\n",
    "date_time = datetime.datetime.today().strftime(\"%d_%m_%Y\")\n",
    "# list where all plots land\n",
    "all_plots=[]\n",
    "# list where \n",
    "#iterate over keyword list\n",
    "for keyword in keyword_list:\n",
    "    # get series for news\n",
    "    news_data = occurence_df[keyword]\n",
    "    # get series for google\n",
    "    google_data = dfs[keyword]\n",
    "    \n",
    "    # rename the Series\n",
    "    news_data= news_data.rename(\"News_Data\")\n",
    "    google_data = google_data.rename(\"Google_Data\")\n",
    "    \n",
    "    # cast to dataframe \n",
    "    news_data = news_data.to_frame()\n",
    "    google_data = google_data.to_frame()\n",
    "    # reset index\n",
    "    news_data = news_data.reset_index()\n",
    "    \n",
    "    news_data['date'] = pd.to_datetime(news_data.date) \n",
    "    merged_data = pd.merge(news_data,google_data, how=\"outer\", on=\"date\")\n",
    "    merged_data.to_csv(\"/Users/jan/Documents/Python_Projects/Bachelorthesis/Analysis/News_vs_Google/news_vs_google\"+date_time+\"_\"+keyword +\".csv\",index=False)\n",
    "    all_plots.append(createFigure(merged_data,keyword))\n",
    "\n",
    "\n",
    "# save all figures in one pdf\n",
    "with PdfPages(\"/Users/jan/Documents/Python_Projects/Bachelorthesis/Analysis/News_vs_Google/news_vs_google\"+date_time +\".pdf\") as pdf:\n",
    "    for plot in all_plots:\n",
    "        pdf.savefig(plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6cb285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFigure(data_frame, fileName):\n",
    "    plot = data_frame.plot.line(x=\"date\",y=[1,2],figsize=(20,10), title=\"Occurence of \"+fileName)\n",
    "    Correlation = data_frame[\"News_Data\"].corr(data_frame[\"Google_Data\"])\n",
    "    plot.annotate(\"Correlation: \" + str(Correlation) , xy=(0.05, 0.95), xycoords='axes fraction')    \n",
    "    return plot.get_figure()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd6958",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e7fe5",
   "metadata": {},
   "source": [
    "### Plot occurence of top 50 titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aae462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all titles\n",
    "all_titles = get_occurence_of_all_titles(All_articles,\"tokenized_titles\")\n",
    "\n",
    "# get top 50\n",
    "top50_titles = all_titles[:50]\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "top50_titles.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653fd5e",
   "metadata": {},
   "source": [
    "### Plot occurence of top 50 titles starting with capital letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all titles\n",
    "all_titles = get_occurence_of_all_capital_titles(All_articles,\"tokenized_titles\")\n",
    "\n",
    "#get top 50\n",
    "top50_titles = all_titles[:50]\n",
    "\n",
    "#plot\n",
    "top50_titles.plot.bar(x=\"Title\",y=\"Occurence\",figsize=(20, 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
