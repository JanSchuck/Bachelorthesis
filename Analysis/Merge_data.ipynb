{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataframes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\AppData\\Local\\Temp\\ipykernel_10272\\3557544987.py:7: DtypeWarning: Columns (1,5,6,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  News_DataFrame = pd.read_csv(path+filename_news, index_col=None,header=0)\n"
     ]
    }
   ],
   "source": [
    "# Load exploded dataframe\n",
    "path = \"C:/Users/Jan/Documents/Python_Projects/Bachelorthesis/Bachelorthesis/Analysis/DataFrames/\"\n",
    "filename_news = \"All_news_articles_exploded.csv\"\n",
    "filename_Google = \"Google_DataFrame.csv\"\n",
    "filename_wiki = \"Wikipedia_DataFrame.csv\"\n",
    "\n",
    "News_DataFrame = pd.read_csv(path+filename_news, index_col=None,header=0)\n",
    "News_DataFrame[\"Date_Info\"] = pd.to_datetime(News_DataFrame[\"Date_Info\"], format='%Y-%m-%d')\n",
    "\n",
    "Google_DataFrame = pd.read_csv(path+filename_Google, index_col=None,header=0)\n",
    "Wiki_DataFrame = pd.read_csv(path+filename_wiki, index_col=None,header=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build occurence dataFrame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Occurence dataframe by media house and combined"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "media_houses =[\"Spiegel\", \"Bild\",\"Sueddeutsche\"]\n",
    "\n",
    "df = build_Occurence_df(News_DataFrame,50, Wiki_DataFrame, Google_DataFrame)\n",
    "saveCSV(df,\"Occurence_DataFrame\")\n",
    "for media_house in media_houses:\n",
    "    News_DataFrame_ = News_DataFrame[News_DataFrame[\"News_page\"] == media_house]\n",
    "    df = build_Occurence_df(News_DataFrame_,50, Wiki_DataFrame, Google_DataFrame)\n",
    "    saveCSV(df,f\"Occurence_DataFrame_{media_house}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Occurence dataframe by season"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "winter_start = datetime.datetime(2022, 1, 10)\n",
    "winter_end = datetime.datetime(2022,3,19)\n",
    "winter_mask = (News_DataFrame['Date_Info'] > winter_start) & (News_DataFrame['Date_Info'] <= winter_end)\n",
    "\n",
    "spring_start = datetime.datetime(2022,3,20)\n",
    "spring_end = datetime.datetime(2022,6,20)\n",
    "spring_mask = (News_DataFrame['Date_Info'] > spring_start) & (News_DataFrame['Date_Info'] <= spring_end)\n",
    "\n",
    "summer_start =datetime.datetime(2022,6,21)\n",
    "summer_end = datetime.datetime(2022,7,7)\n",
    "summer_mask = (News_DataFrame['Date_Info'] > summer_start) & (News_DataFrame['Date_Info'] <= summer_end)\n",
    "\n",
    "News_DataFrame_Winter = News_DataFrame.loc[winter_mask]\n",
    "News_DataFrame_Spring = News_DataFrame.loc[spring_mask]\n",
    "News_DataFrame_Summer = News_DataFrame.loc[summer_mask]\n",
    "\n",
    "season_masks = [[winter_mask,\"winter\"],[spring_mask,\"spring\"],[summer_mask,\"summer\"]]\n",
    "\n",
    "for season_mask,season in season_masks:\n",
    "    df = build_Occurence_df(News_DataFrame.loc[season_mask],50,Wiki_DataFrame,Google_DataFrame)\n",
    "    saveCSV(df, f\"Occurence_DataFrame_{season}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def build_Occurence_df(news_dataFrame, n_occurences, wikipedia_DataFrame, google_DataFrame):\n",
    "    article_over_occurence = get_titles_with_minimum_occurence_N(news_dataFrame[[\"Date_Info\", \"Tokens\",\"Kategorie\"]], n_occurences)\n",
    "\n",
    "    # prep keyword list \"Titles\"\n",
    "    keyword_list = list(article_over_occurence[\"Tokens\"].drop_duplicates())\n",
    "    # get google data\n",
    "    google_data = google_DataFrame.copy()\n",
    "    # get wikipedia data\n",
    "    wikipedia_data = wikipedia_DataFrame.copy()\n",
    "\n",
    "    #rename columns\n",
    "    article_over_occurence = article_over_occurence.rename(columns={\"Date_Info\" : \"date\", \"Tokens\" : \"Occurence_in_news\"})\n",
    "    #convert to datetime\n",
    "    article_over_occurence['date'] = pd.to_datetime(article_over_occurence.date)\n",
    "    google_data['date'] = pd.to_datetime(google_data.date)\n",
    "    wikipedia_data['date'] = pd.to_datetime(wikipedia_data.date)\n",
    "    #convert datetime format to googles datetime format\n",
    "    article_over_occurence['date'] = article_over_occurence['date'].dt.strftime('%Y-%m-%d')\n",
    "    #reshape our data frame to look like google data frame\n",
    "    occurence_df=article_over_occurence.pivot_table(index='date', columns='Occurence_in_news', aggfunc='size').rename_axis(None, axis=1)\n",
    "    #fill all NaN with 0\n",
    "    occurence_df = occurence_df.fillna(0)\n",
    "    occurence_df = occurence_df.astype(int)\n",
    "\n",
    "    # bring news df to right format\n",
    "    news_df = occurence_df.unstack().reset_index()\n",
    "    news_df = news_df.rename(columns={\"level_0\" : \"KeyWord\", 0 : \"Occurence_in_News\"})\n",
    "    news_df[\"date\"]=pd.to_datetime(news_df[\"date\"], format='%Y-%m-%d')\n",
    "    news_df = normalize_column_by_keyword(news_df,keyword_list,\"Occurence_in_News\")\n",
    "\n",
    "    # merge google and news\n",
    "    #full_df = pd.merge(news_df, google_data, on=['KeyWord',\"date\"], how='outer')\n",
    "\n",
    "    full_df = pd.merge(pd.merge(news_df,google_data,on=['KeyWord',\"date\"]),wikipedia_data,on=['KeyWord',\"date\"])\n",
    "\n",
    "\n",
    "    #fill all NaN with 0\n",
    "    full_df[\"Occurence_in_Google\"] = full_df[\"Occurence_in_Google\"].fillna(0)\n",
    "    full_df[\"Occurence_in_Google\"] = full_df[\"Occurence_in_Google\"].astype(int)\n",
    "\n",
    "    full_df = full_df.groupby(\"KeyWord\").apply(smoothen_timeseries)\n",
    "\n",
    "    return full_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def smoothen_timeseries(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    window_size = 3\n",
    "    normalized_Occurence_column_names = [\"normalized_Occurence_in_News\",\"normalized_Occurence_in_Google\",\n",
    "                                         \"normalized_Occurence_in_Wikipedia\"]\n",
    "    Occurence_column_names = [\"Occurence_in_News\",\"Occurence_in_Google\",\"Occurence_in_Wikipedia\"]\n",
    "\n",
    "    Column_names = normalized_Occurence_column_names + Occurence_column_names\n",
    "\n",
    "    for column_name in Column_names:\n",
    "        # smoothing Occurence\n",
    "\n",
    "        data = df[column_name].rolling(window_size).mean().fillna(0)\n",
    "        dataframe[\"smoothened_\"+column_name] = data\n",
    "    return dataframe\n",
    "\n",
    "def get_occurence_of_all_titles(data_frame, columnName):\n",
    "    # Group titles by columnName\n",
    "    all_titles = data_frame.groupby(columnName).size()\n",
    "\n",
    "    #sort titles\n",
    "    all_titles = all_titles.sort_values(ascending = False)\n",
    "\n",
    "    return all_titles\n",
    "\n",
    "\n",
    "def get_occurence_of_all_capital_titles(data_frame, columnName):\n",
    "    all_titles=data_frame.groupby(columnName).size()\n",
    "\n",
    "    capital_titles=[]\n",
    "    counter=0\n",
    "    for title in all_titles.items():\n",
    "        title_s=str(title)\n",
    "        title_s=title_s.strip()\n",
    "        if title_s.istitle():\n",
    "            capital_titles.append(title)\n",
    "    Titles = pd.DataFrame.from_records(\n",
    "    capital_titles, columns=['Title','Occurence'])\n",
    "    return Titles.sort_values(by=['Occurence'],ascending=False)\n",
    "\n",
    "\n",
    "def get_titles_with_minimum_occurence_N(data_frame, N):\n",
    "    all_Titles = get_occurence_of_all_capital_titles(data_frame, \"Tokens\")\n",
    "    above_N = []\n",
    "    for index,row in all_Titles.iterrows():\n",
    "        if int(row.Occurence) >= N:\n",
    "            above_N.append(row)\n",
    "\n",
    "    above_N = pd.DataFrame(above_N, columns=['Title', 'Occurence'])\n",
    "\n",
    "    return data_frame[data_frame[\"Tokens\"].isin(above_N.Title)]\n",
    "\n",
    "\n",
    "def get_titles_with_minimum_occurence_N(data_frame, N):\n",
    "    all_Titles = get_occurence_of_all_capital_titles(data_frame, \"Tokens\")\n",
    "    above_N = []\n",
    "    for index,row in all_Titles.iterrows():\n",
    "        if int(row.Occurence) >= N:\n",
    "            above_N.append(row)\n",
    "\n",
    "    above_N = pd.DataFrame(above_N, columns=['Title', 'Occurence'])\n",
    "\n",
    "    return data_frame[data_frame[\"Tokens\"].isin(above_N.Title)]\n",
    "\n",
    "\n",
    "def normalize_column_by_keyword(dataframe, keyword_list, column):\n",
    "    dataframe_list = []\n",
    "    new_column_name = \"normalized_\" + column\n",
    "    for keyword in keyword_list:\n",
    "        working_df = dataframe[dataframe['KeyWord'] == keyword]\n",
    "        max_occurence = working_df[column].max()\n",
    "        #print(max_occurence)\n",
    "        df_copy = working_df.copy()\n",
    "        df_copy[new_column_name] = working_df[column] /working_df[column].abs().max()\n",
    "        dataframe_list.append(df_copy)\n",
    "    return pd.concat(dataframe_list)\n",
    "\n",
    "\n",
    "def saveCSV(dataframe, filename):\n",
    "    dataframe.to_csv(\"C:/Users/Jan/Documents/Python_Projects/Bachelorthesis/Bachelorthesis/Analysis/DataFrames/\"+ filename +\".csv\",index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}